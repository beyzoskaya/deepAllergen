{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1zlFsGeHMoOuXQFw7tLJ-D1n32xWyCuD0",
      "authorship_tag": "ABX9TyOOvWK2i80yqg7ke+RaXju8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/beyzoskaya/deepAllergen/blob/main/deepAllergen.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sgtDM87c3GfF",
        "outputId": "3883ae5e-c9c2-4f18-e1fc-19fbb61c1d54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (5.0.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (0.2.1)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu128)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.3)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (1.4.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (26.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.2)\n",
            "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.3)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.12.19)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.7.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (5.29.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.1.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.76.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.15.1)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.46.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (0.28.1)\n",
            "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.5.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.18.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2026.1.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.10.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim->transformers) (8.3.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (4.12.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (0.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers sentencepiece tensorflow torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install biopython"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yeu3cUfQbeBI",
        "outputId": "552e4c87-894d-48da-9b81-53ce7ff788b5"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: biopython in /usr/local/lib/python3.12/dist-packages (1.86)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from biopython) (2.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update\n",
        "!apt-get install -y cd-hit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sIiuaqqMbqOq",
        "outputId": "1eced6aa-49a6-4d0a-bab8-295fe02ff014"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com] [Connected to cloud.r-project.org (99.84.\r                                                                               \rHit:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.91.83)] [Waiting for headers] [Wai\r                                                                               \rHit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.91.83)] [Waiting for headers] [Con\r                                                                               \rGet:4 https://cli.github.com/packages stable InRelease [3,917 B]\n",
            "\r0% [Waiting for headers] [Waiting for headers] [Connecting to ppa.launchpadcont\r0% [Waiting for headers] [Waiting for headers] [Connecting to ppa.launchpadcont\r                                                                               \rHit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "\r0% [Waiting for headers] [Waiting for headers] [Connecting to ppa.launchpadcont\r                                                                               \rHit:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "\r0% [Waiting for headers] [Connecting to ppa.launchpadcontent.net (185.125.190.8\r0% [Waiting for headers] [Waiting for headers] [Connecting to ppa.launchpadcont\r                                                                               \rHit:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:8 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Fetched 3,917 B in 1s (4,137 B/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "cd-hit is already the newest version (4.8.1-4).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 48 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from Bio import SeqIO\n",
        "\n",
        "input_files = [\"all.train.fasta\", \"all.test.fasta\"]\n",
        "\n",
        "records = []\n",
        "seen_sequences = set()\n",
        "\n",
        "for file in input_files:\n",
        "    for rec in SeqIO.parse(file, \"fasta\"):\n",
        "        seq = str(rec.seq)\n",
        "        if seq not in seen_sequences:\n",
        "            seen_sequences.add(seq)\n",
        "            records.append(rec)\n",
        "\n",
        "SeqIO.write(records, \"all_merged_nr_exact.fasta\", \"fasta\")\n",
        "\n",
        "print(\"Total unique sequences:\", len(records))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wv1pE1jFbuHp",
        "outputId": "c9a93c33-1288-4e96-cfa9-e3a2584a9950"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total unique sequences: 7125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd-hit -i all_merged_nr_exact.fasta -o all_clusters.fasta -c 0.5 -n 3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cw3X0dCZcMSs",
        "outputId": "40e4aa37-3bda-4ea5-95cf-066592b3dedd"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================\n",
            "Program: CD-HIT, V4.8.1 (+OpenMP), Aug 20 2021, 08:39:56\n",
            "Command: cd-hit -i all_merged_nr_exact.fasta -o\n",
            "         all_clusters.fasta -c 0.5 -n 3\n",
            "\n",
            "Started: Sun Feb 15 17:54:44 2026\n",
            "================================================================\n",
            "                            Output                              \n",
            "----------------------------------------------------------------\n",
            "Warning: from file \"all_merged_nr_exact.fasta\",\n",
            "Discarding invalid sequence or sequence without identifier and description!\n",
            "\n",
            ">allergen_Triosephos\n",
            "MANQRKFFVGGNWKMNGDRAGIDSIISFMKGPLSADTEVVVGCPQCYLMYTREHLPSNIG\n",
            "VAAQNCYKVAKGAFTGEISPSMIKDCGCEWVILGHSERRNVFNEPDTLISEKVGHALEAG\n",
            "LKVIPCIGEKLEERESNRTEEVVFAQMKALVPNISDWSRVVIAYEPVWAIGTGKTATPEQ\n",
            "AQEVHAKLRQWLRDNVNAEVADSTRIIYGGSVTPGNCKELAKTGDIDGFLVGGASLKPDF\n",
            "VQIINARD>allergen_TriosephosphateIsomerase10MANQRKFFVGGNWKMN\n",
            "GDKAAIDGIISFMKGPLNADTEVVVGCPQCYLMYTREHMPANIGVAAQNCYKTAKGAFTG\n",
            "EISPAMIKDCGCEWVILGHSERRNVFGEPDQLISEKVGHALEAGLKVIPCIGEKLEERES\n",
            "NRTEEVVFAQMKALVPNISDWSRVVIAYEPVWAIGTGKTATPEQAQDVHAKLRQWLRDNV\n",
            "SPQVAESTRIIYGGSVSAGNCKELAKTGDIDGFLVGGASLKPDFVTIINARA\n",
            "total seq: 7124\n",
            "longest and shortest : 1388 and 11\n",
            "Total letters: 2235422\n",
            "Sequences have been sorted\n",
            "\n",
            "Approximated minimal memory consumption:\n",
            "Sequence        : 3M\n",
            "Buffer          : 1 X 10M = 10M\n",
            "Table           : 1 X 0M = 0M\n",
            "Miscellaneous   : 0M\n",
            "Total           : 14M\n",
            "\n",
            "Table limit with the given memory limit:\n",
            "Max number of representatives: 1509661\n",
            "Max number of word counting entries: 98201232\n",
            "\n",
            "\rcomparing sequences from          0  to       7124\n",
            ".......\n",
            "     7124  finished       4337  clusters\n",
            "\n",
            "Approximated maximum memory consumption: 26M\n",
            "writing new database\n",
            "writing clustering information\n",
            "program completed !\n",
            "\n",
            "Total CPU time 59.96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from Bio import SeqIO\n",
        "from collections import defaultdict\n",
        "\n",
        "random.seed(42)\n",
        "\n",
        "CLUSTER_FILE = \"all_clusters.fasta.clstr\"\n",
        "FASTA_FILE = \"all_merged_nr_exact.fasta\"\n",
        "\n",
        "TRAIN_RATIO = 0.70\n",
        "VAL_RATIO = 0.15\n",
        "TEST_RATIO = 0.15\n",
        "\n",
        "# -----------------------------\n",
        "# Parse clusters\n",
        "# -----------------------------\n",
        "clusters = defaultdict(list)\n",
        "cluster_id = None\n",
        "\n",
        "with open(CLUSTER_FILE) as f:\n",
        "    for line in f:\n",
        "        line = line.strip()\n",
        "\n",
        "        if line.startswith(\">Cluster\"):\n",
        "            cluster_id = line\n",
        "        else:\n",
        "            seq_id = line.split(\">\")[1].split(\"...\")[0]\n",
        "            clusters[cluster_id].append(seq_id)\n",
        "\n",
        "print(\"Total clusters:\", len(clusters))\n",
        "\n",
        "# -----------------------------\n",
        "# Load sequences\n",
        "# -----------------------------\n",
        "seq_dict = {rec.id: rec for rec in SeqIO.parse(FASTA_FILE, \"fasta\")}\n",
        "\n",
        "# -----------------------------\n",
        "# Shuffle clusters\n",
        "# -----------------------------\n",
        "cluster_list = list(clusters.values())\n",
        "random.shuffle(cluster_list)\n",
        "\n",
        "# -----------------------------\n",
        "# Assign clusters\n",
        "# -----------------------------\n",
        "total_sequences = sum(len(c) for c in cluster_list)\n",
        "\n",
        "train_ids = set()\n",
        "val_ids = set()\n",
        "test_ids = set()\n",
        "\n",
        "count_train = 0\n",
        "count_val = 0\n",
        "\n",
        "for cluster in cluster_list:\n",
        "\n",
        "    if count_train / total_sequences < TRAIN_RATIO:\n",
        "        train_ids.update(cluster)\n",
        "        count_train += len(cluster)\n",
        "\n",
        "    elif count_val / total_sequences < VAL_RATIO:\n",
        "        val_ids.update(cluster)\n",
        "        count_val += len(cluster)\n",
        "\n",
        "    else:\n",
        "        test_ids.update(cluster)\n",
        "\n",
        "# -----------------------------\n",
        "# Save FASTA\n",
        "# -----------------------------\n",
        "def save_split(ids, filename):\n",
        "    records = [seq_dict[i] for i in ids if i in seq_dict]\n",
        "    SeqIO.write(records, filename, \"fasta\")\n",
        "    print(f\"{filename}: {len(records)} sequences\")\n",
        "\n",
        "save_split(train_ids, \"train_safe.fasta\")\n",
        "save_split(val_ids, \"val_safe.fasta\")\n",
        "save_split(test_ids, \"test_safe.fasta\")\n",
        "\n",
        "# -----------------------------\n",
        "# Leakage check\n",
        "# -----------------------------\n",
        "print(\"\\nLeakage Check\")\n",
        "print(\"Train âˆ© Val:\", len(train_ids & val_ids))\n",
        "print(\"Train âˆ© Test:\", len(train_ids & test_ids))\n",
        "print(\"Val âˆ© Test:\", len(val_ids & test_ids))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zt8C1bXqcV3o",
        "outputId": "ae715f6e-017c-4c30-c036-9b4806abbbc3"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total clusters: 4337\n",
            "train_safe.fasta: 5021 sequences\n",
            "val_safe.fasta: 1069 sequences\n",
            "test_safe.fasta: 1020 sequences\n",
            "\n",
            "Leakage Check\n",
            "Train âˆ© Val: 0\n",
            "Train âˆ© Test: 0\n",
            "Val âˆ© Test: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from Bio import SeqIO\n",
        "\n",
        "for f in [\"train_safe.fasta\",\"val_safe.fasta\",\"test_safe.fasta\"]:\n",
        "    count = sum(1 for _ in SeqIO.parse(f, \"fasta\"))\n",
        "    print(f\"{f}: {count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VK3q68UXccBI",
        "outputId": "e828132a-ac33-49af-c568-58554f580a56"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_safe.fasta: 5021\n",
            "val_safe.fasta: 1069\n",
            "test_safe.fasta: 1020\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python datapreprocess.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EOz0lHG73Ocq",
        "outputId": "8e6fb75b-b022-4b13-b08d-30d0decfc786"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2026-02-15 15:18:13.003097: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1771168693.022952    1527 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1771168693.029712    1527 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1771168693.045258    1527 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1771168693.045283    1527 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1771168693.045287    1527 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1771168693.045292    1527 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2026-02-15 15:18:13.049719: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "ðŸ“± Device: cuda\n",
            "â³ Loading ProtT5 model: Rostlab/prot_t5_xl_uniref50\n",
            "tokenizer_config.json: 100% 24.0/24.0 [00:00<00:00, 103kB/s]\n",
            "spiece.model: 100% 238k/238k [00:00<00:00, 618kB/s]\n",
            "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
            "special_tokens_map.json: 1.79kB [00:00, 7.03MB/s]\n",
            "config.json: 100% 546/546 [00:00<00:00, 4.05MB/s]\n",
            "pytorch_model.bin: 100% 11.3G/11.3G [02:00<00:00, 93.2MB/s]\n",
            "model.safetensors:  45% 5.11G/11.3G [01:18<01:08, 89.4MB/s]\n",
            "Loading weights:   0% 0/196 [00:00<?, ?it/s]\u001b[A\n",
            "Loading weights:   1% 1/196 [00:00<00:00, 10155.70it/s, Materializing param=encoder.block.0.layer.0.SelfAttention.k.weight]\u001b[A\n",
            "Loading weights:   1% 1/196 [00:00<00:00, 5035.18it/s, Materializing param=encoder.block.0.layer.0.SelfAttention.k.weight] \u001b[A\n",
            "Loading weights:   1% 2/196 [00:00<00:00, 4639.72it/s, Materializing param=encoder.block.0.layer.0.SelfAttention.o.weight]\u001b[A\n",
            "Loading weights:   1% 2/196 [00:00<00:00, 3806.08it/s, Materializing param=encoder.block.0.layer.0.SelfAttention.o.weight]\u001b[A\n",
            "Loading weights:   2% 3/196 [00:00<00:00, 4215.38it/s, Materializing param=encoder.block.0.layer.0.SelfAttention.q.weight]\u001b[A\n",
            "Loading weights:   2% 3/196 [00:00<00:00, 3661.02it/s, Materializing param=encoder.block.0.layer.0.SelfAttention.q.weight]\u001b[A\n",
            "Loading weights:   2% 4/196 [00:00<00:00, 3958.76it/s, Materializing param=encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight]\u001b[A\n",
            "Loading weights:   2% 4/196 [00:00<00:00, 3599.49it/s, Materializing param=encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight]\u001b[A\n",
            "Loading weights:   3% 5/196 [00:00<00:00, 3897.33it/s, Materializing param=encoder.block.0.layer.0.SelfAttention.v.weight]                      \u001b[A\n",
            "Loading weights:   3% 5/196 [00:00<00:00, 3661.23it/s, Materializing param=encoder.block.0.layer.0.SelfAttention.v.weight]\u001b[A\n",
            "Loading weights:   3% 6/196 [00:00<00:00, 3945.72it/s, Materializing param=encoder.block.0.layer.0.layer_norm.weight]     \u001b[A\n",
            "Loading weights:   3% 6/196 [00:00<00:00, 3746.59it/s, Materializing param=encoder.block.0.layer.0.layer_norm.weight]\u001b[A\n",
            "Loading weights:   4% 7/196 [00:00<00:00, 3999.47it/s, Materializing param=encoder.block.0.layer.1.DenseReluDense.wi.weight]\u001b[A\n",
            "Loading weights:   4% 7/196 [00:00<00:00, 3773.80it/s, Materializing param=encoder.block.0.layer.1.DenseReluDense.wi.weight]\u001b[A\n",
            "Loading weights:   4% 8/196 [00:00<00:00, 3941.55it/s, Materializing param=encoder.block.0.layer.1.DenseReluDense.wo.weight]\u001b[A\n",
            "Loading weights:   4% 8/196 [00:00<00:00, 3762.55it/s, Materializing param=encoder.block.0.layer.1.DenseReluDense.wo.weight]\u001b[A\n",
            "Loading weights:   5% 9/196 [00:00<00:00, 3947.79it/s, Materializing param=encoder.block.0.layer.1.layer_norm.weight]       \u001b[A\n",
            "Loading weights:   5% 9/196 [00:00<00:00, 3815.32it/s, Materializing param=encoder.block.0.layer.1.layer_norm.weight]\u001b[A\n",
            "Loading weights:   5% 10/196 [00:00<00:00, 3951.30it/s, Materializing param=encoder.block.1.layer.0.SelfAttention.k.weight]\u001b[A\n",
            "Loading weights:   5% 10/196 [00:00<00:00, 3824.48it/s, Materializing param=encoder.block.1.layer.0.SelfAttention.k.weight]\u001b[A\n",
            "Loading weights:   6% 11/196 [00:00<00:00, 3950.12it/s, Materializing param=encoder.block.1.layer.0.SelfAttention.o.weight]\u001b[A\n",
            "Loading weights:   6% 11/196 [00:00<00:00, 3825.02it/s, Materializing param=encoder.block.1.layer.0.SelfAttention.o.weight]\u001b[A\n",
            "Loading weights:   6% 12/196 [00:00<00:00, 3905.31it/s, Materializing param=encoder.block.1.layer.0.SelfAttention.q.weight]\u001b[A\n",
            "Loading weights:   6% 12/196 [00:00<00:00, 3795.46it/s, Materializing param=encoder.block.1.layer.0.SelfAttention.q.weight]\u001b[A\n",
            "Loading weights:   7% 13/196 [00:00<00:00, 3893.60it/s, Materializing param=encoder.block.1.layer.0.SelfAttention.v.weight]\u001b[A\n",
            "Loading weights:   7% 13/196 [00:00<00:00, 3761.97it/s, Materializing param=encoder.block.1.layer.0.SelfAttention.v.weight]\u001b[A\n",
            "Loading weights:   7% 14/196 [00:00<00:00, 3853.29it/s, Materializing param=encoder.block.1.layer.0.layer_norm.weight]     \u001b[A\n",
            "Loading weights:   7% 14/196 [00:00<00:00, 3768.71it/s, Materializing param=encoder.block.1.layer.0.layer_norm.weight]\u001b[A\n",
            "Loading weights:   8% 15/196 [00:00<00:00, 3870.00it/s, Materializing param=encoder.block.1.layer.1.DenseReluDense.wi.weight]\u001b[A\n",
            "Loading weights:   8% 15/196 [00:00<00:00, 3773.67it/s, Materializing param=encoder.block.1.layer.1.DenseReluDense.wi.weight]\u001b[A\n",
            "Loading weights:   8% 16/196 [00:00<00:00, 3858.38it/s, Materializing param=encoder.block.1.layer.1.DenseReluDense.wo.weight]\u001b[A\n",
            "Loading weights:   8% 16/196 [00:00<00:00, 3779.72it/s, Materializing param=encoder.block.1.layer.1.DenseReluDense.wo.weight]\u001b[A\n",
            "Loading weights:   9% 17/196 [00:00<00:00, 3869.28it/s, Materializing param=encoder.block.1.layer.1.layer_norm.weight]       \u001b[A\n",
            "Loading weights:   9% 17/196 [00:00<00:00, 3796.76it/s, Materializing param=encoder.block.1.layer.1.layer_norm.weight]\u001b[A\n",
            "Loading weights:   9% 18/196 [00:00<00:00, 3881.42it/s, Materializing param=encoder.block.2.layer.0.SelfAttention.k.weight]\u001b[A\n",
            "Loading weights:   9% 18/196 [00:00<00:00, 3801.68it/s, Materializing param=encoder.block.2.layer.0.SelfAttention.k.weight]\u001b[A\n",
            "Loading weights:  10% 19/196 [00:00<00:00, 3857.48it/s, Materializing param=encoder.block.2.layer.0.SelfAttention.o.weight]\u001b[A\n",
            "Loading weights:  10% 19/196 [00:00<00:00, 3784.57it/s, Materializing param=encoder.block.2.layer.0.SelfAttention.o.weight]\u001b[A\n",
            "Loading weights:  10% 20/196 [00:00<00:00, 3849.57it/s, Materializing param=encoder.block.2.layer.0.SelfAttention.q.weight]\u001b[A\n",
            "Loading weights:  10% 20/196 [00:00<00:00, 3788.89it/s, Materializing param=encoder.block.2.layer.0.SelfAttention.q.weight]\u001b[A\n",
            "Loading weights:  11% 21/196 [00:00<00:00, 3867.08it/s, Materializing param=encoder.block.2.layer.0.SelfAttention.v.weight]\u001b[A\n",
            "Loading weights:  11% 21/196 [00:00<00:00, 3810.20it/s, Materializing param=encoder.block.2.layer.0.SelfAttention.v.weight]\u001b[A\n",
            "Loading weights:  11% 22/196 [00:00<00:00, 3886.40it/s, Materializing param=encoder.block.2.layer.0.layer_norm.weight]     \u001b[A\n",
            "Loading weights:  11% 22/196 [00:00<00:00, 3833.28it/s, Materializing param=encoder.block.2.layer.0.layer_norm.weight]\u001b[A\n",
            "Loading weights:  12% 23/196 [00:00<00:00, 3899.47it/s, Materializing param=encoder.block.2.layer.1.DenseReluDense.wi.weight]\u001b[A\n",
            "Loading weights:  12% 23/196 [00:00<00:00, 3845.99it/s, Materializing param=encoder.block.2.layer.1.DenseReluDense.wi.weight]\u001b[A\n",
            "Loading weights:  12% 24/196 [00:00<00:00, 3910.17it/s, Materializing param=encoder.block.2.layer.1.DenseReluDense.wo.weight]\u001b[A\n",
            "Loading weights:  12% 24/196 [00:00<00:00, 3859.49it/s, Materializing param=encoder.block.2.layer.1.DenseReluDense.wo.weight]\u001b[A\n",
            "Loading weights:  13% 25/196 [00:00<00:00, 3924.90it/s, Materializing param=encoder.block.2.layer.1.layer_norm.weight]       \u001b[A\n",
            "Loading weights:  13% 25/196 [00:00<00:00, 3876.58it/s, Materializing param=encoder.block.2.layer.1.layer_norm.weight]\u001b[A\n",
            "Loading weights:  13% 26/196 [00:00<00:00, 3943.44it/s, Materializing param=encoder.block.3.layer.0.SelfAttention.k.weight]\u001b[A\n",
            "Loading weights:  13% 26/196 [00:00<00:00, 3896.10it/s, Materializing param=encoder.block.3.layer.0.SelfAttention.k.weight]\u001b[A\n",
            "Loading weights:  14% 27/196 [00:00<00:00, 3939.96it/s, Materializing param=encoder.block.3.layer.0.SelfAttention.o.weight]\u001b[A\n",
            "Loading weights:  14% 27/196 [00:00<00:00, 3891.62it/s, Materializing param=encoder.block.3.layer.0.SelfAttention.o.weight]\u001b[A\n",
            "Loading weights:  14% 28/196 [00:00<00:00, 3938.18it/s, Materializing param=encoder.block.3.layer.0.SelfAttention.q.weight]\u001b[A\n",
            "Loading weights:  14% 28/196 [00:00<00:00, 3893.79it/s, Materializing param=encoder.block.3.layer.0.SelfAttention.q.weight]\u001b[A\n",
            "Loading weights:  15% 29/196 [00:00<00:00, 3949.57it/s, Materializing param=encoder.block.3.layer.0.SelfAttention.v.weight]\u001b[A\n",
            "Loading weights:  15% 29/196 [00:00<00:00, 3907.69it/s, Materializing param=encoder.block.3.layer.0.SelfAttention.v.weight]\u001b[A\n",
            "Loading weights:  15% 30/196 [00:00<00:00, 3965.12it/s, Materializing param=encoder.block.3.layer.0.layer_norm.weight]     \u001b[A\n",
            "Loading weights:  15% 30/196 [00:00<00:00, 3924.80it/s, Materializing param=encoder.block.3.layer.0.layer_norm.weight]\u001b[A\n",
            "Loading weights:  16% 31/196 [00:00<00:00, 3971.39it/s, Materializing param=encoder.block.3.layer.1.DenseReluDense.wi.weight]\u001b[A\n",
            "Loading weights:  16% 31/196 [00:00<00:00, 3929.98it/s, Materializing param=encoder.block.3.layer.1.DenseReluDense.wi.weight]\u001b[A\n",
            "Loading weights:  16% 32/196 [00:00<00:00, 3972.35it/s, Materializing param=encoder.block.3.layer.1.DenseReluDense.wo.weight]\u001b[A\n",
            "Loading weights:  16% 32/196 [00:00<00:00, 3931.28it/s, Materializing param=encoder.block.3.layer.1.DenseReluDense.wo.weight]\u001b[A\n",
            "Loading weights:  17% 33/196 [00:00<00:00, 3981.13it/s, Materializing param=encoder.block.3.layer.1.layer_norm.weight]       \u001b[A\n",
            "Loading weights:  17% 33/196 [00:00<00:00, 3944.04it/s, Materializing param=encoder.block.3.layer.1.layer_norm.weight]\u001b[A\n",
            "Loading weights:  17% 34/196 [00:00<00:00, 3995.25it/s, Materializing param=encoder.block.4.layer.0.SelfAttention.k.weight]\u001b[A\n",
            "Loading weights:  17% 34/196 [00:00<00:00, 3959.53it/s, Materializing param=encoder.block.4.layer.0.SelfAttention.k.weight]\u001b[A\n",
            "Loading weights:  18% 35/196 [00:00<00:00, 4008.54it/s, Materializing param=encoder.block.4.layer.0.SelfAttention.o.weight]\u001b[A\n",
            "Loading weights:  18% 35/196 [00:00<00:00, 3969.09it/s, Materializing param=encoder.block.4.layer.0.SelfAttention.o.weight]\u001b[A\n",
            "Loading weights:  18% 36/196 [00:00<00:00, 4001.77it/s, Materializing param=encoder.block.4.layer.0.SelfAttention.q.weight]\u001b[A\n",
            "Loading weights:  18% 36/196 [00:00<00:00, 3961.98it/s, Materializing param=encoder.block.4.layer.0.SelfAttention.q.weight]\u001b[A\n",
            "Loading weights:  19% 37/196 [00:00<00:00, 4004.88it/s, Materializing param=encoder.block.4.layer.0.SelfAttention.v.weight]\u001b[A\n",
            "Loading weights:  19% 37/196 [00:00<00:00, 3970.86it/s, Materializing param=encoder.block.4.layer.0.SelfAttention.v.weight]\u001b[A\n",
            "Loading weights:  19% 38/196 [00:00<00:00, 4014.90it/s, Materializing param=encoder.block.4.layer.0.layer_norm.weight]     \u001b[A\n",
            "Loading weights:  19% 38/196 [00:00<00:00, 3982.10it/s, Materializing param=encoder.block.4.layer.0.layer_norm.weight]\u001b[A\n",
            "Loading weights:  20% 39/196 [00:00<00:00, 4027.23it/s, Materializing param=encoder.block.4.layer.1.DenseReluDense.wi.weight]\u001b[A\n",
            "Loading weights:  20% 39/196 [00:00<00:00, 3994.58it/s, Materializing param=encoder.block.4.layer.1.DenseReluDense.wi.weight]\u001b[A\n",
            "Loading weights:  20% 40/196 [00:00<00:00, 4029.40it/s, Materializing param=encoder.block.4.layer.1.DenseReluDense.wo.weight]\u001b[A\n",
            "Loading weights:  20% 40/196 [00:00<00:00, 3997.34it/s, Materializing param=encoder.block.4.layer.1.DenseReluDense.wo.weight]\u001b[A\n",
            "Loading weights:  21% 41/196 [00:00<00:00, 4032.70it/s, Materializing param=encoder.block.4.layer.1.layer_norm.weight]       \u001b[A\n",
            "Loading weights:  21% 41/196 [00:00<00:00, 4002.01it/s, Materializing param=encoder.block.4.layer.1.layer_norm.weight]\u001b[A\n",
            "Loading weights:  21% 42/196 [00:00<00:00, 4043.54it/s, Materializing param=encoder.block.5.layer.0.SelfAttention.k.weight]\u001b[A\n",
            "Loading weights:  21% 42/196 [00:00<00:00, 4012.87it/s, Materializing param=encoder.block.5.layer.0.SelfAttention.k.weight]\u001b[A\n",
            "Loading weights:  22% 43/196 [00:00<00:00, 4051.38it/s, Materializing param=encoder.block.5.layer.0.SelfAttention.o.weight]\u001b[A\n",
            "Loading weights:  22% 43/196 [00:00<00:00, 4021.21it/s, Materializing param=encoder.block.5.layer.0.SelfAttention.o.weight]\u001b[A\n",
            "Loading weights:  22% 44/196 [00:00<00:00, 4045.89it/s, Materializing param=encoder.block.5.layer.0.SelfAttention.q.weight]\u001b[A\n",
            "Loading weights:  22% 44/196 [00:00<00:00, 4014.74it/s, Materializing param=encoder.block.5.layer.0.SelfAttention.q.weight]\u001b[A\n",
            "Loading weights:  23% 45/196 [00:00<00:00, 4044.91it/s, Materializing param=encoder.block.5.layer.0.SelfAttention.v.weight]\u001b[A\n",
            "Loading weights:  23% 45/196 [00:00<00:00, 4015.57it/s, Materializing param=encoder.block.5.layer.0.SelfAttention.v.weight]\u001b[A\n",
            "Loading weights:  23% 46/196 [00:00<00:00, 4048.73it/s, Materializing param=encoder.block.5.layer.0.layer_norm.weight]     \u001b[A\n",
            "Loading weights:  23% 46/196 [00:00<00:00, 4020.46it/s, Materializing param=encoder.block.5.layer.0.layer_norm.weight]\u001b[A\n",
            "Loading weights:  24% 47/196 [00:00<00:00, 4056.05it/s, Materializing param=encoder.block.5.layer.1.DenseReluDense.wi.weight]\u001b[A\n",
            "Loading weights:  24% 47/196 [00:00<00:00, 4028.12it/s, Materializing param=encoder.block.5.layer.1.DenseReluDense.wi.weight]\u001b[A\n",
            "Loading weights:  24% 48/196 [00:00<00:00, 4056.71it/s, Materializing param=encoder.block.5.layer.1.DenseReluDense.wo.weight]\u001b[A\n",
            "Loading weights:  24% 48/196 [00:00<00:00, 4030.64it/s, Materializing param=encoder.block.5.layer.1.DenseReluDense.wo.weight]\u001b[A\n",
            "Loading weights:  25% 49/196 [00:00<00:00, 4062.56it/s, Materializing param=encoder.block.5.layer.1.layer_norm.weight]       \u001b[A\n",
            "Loading weights:  25% 49/196 [00:00<00:00, 4035.76it/s, Materializing param=encoder.block.5.layer.1.layer_norm.weight]\u001b[A\n",
            "Loading weights:  26% 50/196 [00:00<00:00, 4070.01it/s, Materializing param=encoder.block.6.layer.0.SelfAttention.k.weight]\u001b[A\n",
            "Loading weights:  26% 50/196 [00:00<00:00, 4043.95it/s, Materializing param=encoder.block.6.layer.0.SelfAttention.k.weight]\u001b[A\n",
            "Loading weights:  26% 51/196 [00:00<00:00, 4076.25it/s, Materializing param=encoder.block.6.layer.0.SelfAttention.o.weight]\u001b[A\n",
            "Loading weights:  26% 51/196 [00:00<00:00, 4050.70it/s, Materializing param=encoder.block.6.layer.0.SelfAttention.o.weight]\u001b[A\n",
            "Loading weights:  27% 52/196 [00:00<00:00, 4082.74it/s, Materializing param=encoder.block.6.layer.0.SelfAttention.q.weight]\u001b[A\n",
            "Loading weights:  27% 52/196 [00:00<00:00, 4056.01it/s, Materializing param=encoder.block.6.layer.0.SelfAttention.q.weight]\u001b[A\n",
            "Loading weights:  27% 53/196 [00:00<00:00, 4078.79it/s, Materializing param=encoder.block.6.layer.0.SelfAttention.v.weight]\u001b[A\n",
            "Loading weights:  27% 53/196 [00:00<00:00, 4051.43it/s, Materializing param=encoder.block.6.layer.0.SelfAttention.v.weight]\u001b[A\n",
            "Loading weights:  28% 54/196 [00:00<00:00, 4080.43it/s, Materializing param=encoder.block.6.layer.0.layer_norm.weight]     \u001b[A\n",
            "Loading weights:  28% 54/196 [00:00<00:00, 4056.90it/s, Materializing param=encoder.block.6.layer.0.layer_norm.weight]\u001b[A\n",
            "Loading weights:  28% 55/196 [00:00<00:00, 4086.93it/s, Materializing param=encoder.block.6.layer.1.DenseReluDense.wi.weight]\u001b[A\n",
            "Loading weights:  28% 55/196 [00:00<00:00, 4065.90it/s, Materializing param=encoder.block.6.layer.1.DenseReluDense.wi.weight]\u001b[A\n",
            "Loading weights:  29% 56/196 [00:00<00:00, 4092.50it/s, Materializing param=encoder.block.6.layer.1.DenseReluDense.wo.weight]\u001b[A\n",
            "Loading weights:  29% 56/196 [00:00<00:00, 4065.44it/s, Materializing param=encoder.block.6.layer.1.DenseReluDense.wo.weight]\u001b[A\n",
            "Loading weights:  29% 57/196 [00:00<00:00, 4083.97it/s, Materializing param=encoder.block.6.layer.1.layer_norm.weight]       \u001b[A\n",
            "Loading weights:  29% 57/196 [00:00<00:00, 4059.90it/s, Materializing param=encoder.block.6.layer.1.layer_norm.weight]\u001b[A\n",
            "Loading weights:  30% 58/196 [00:00<00:00, 4074.73it/s, Materializing param=encoder.block.7.layer.0.SelfAttention.k.weight]\u001b[A\n",
            "Loading weights:  30% 58/196 [00:00<00:00, 4050.85it/s, Materializing param=encoder.block.7.layer.0.SelfAttention.k.weight]\u001b[A\n",
            "Loading weights:  30% 59/196 [00:00<00:00, 4066.45it/s, Materializing param=encoder.block.7.layer.0.SelfAttention.o.weight]\u001b[A\n",
            "Loading weights:  30% 59/196 [00:00<00:00, 4043.26it/s, Materializing param=encoder.block.7.layer.0.SelfAttention.o.weight]\u001b[A\n",
            "Loading weights:  31% 60/196 [00:00<00:00, 4064.58it/s, Materializing param=encoder.block.7.layer.0.SelfAttention.q.weight]\u001b[A\n",
            "Loading weights:  31% 60/196 [00:00<00:00, 4039.33it/s, Materializing param=encoder.block.7.layer.0.SelfAttention.q.weight]\u001b[A\n",
            "Loading weights:  31% 61/196 [00:00<00:00, 4053.75it/s, Materializing param=encoder.block.7.layer.0.SelfAttention.v.weight]\u001b[A\n",
            "Loading weights:  31% 61/196 [00:00<00:00, 4028.92it/s, Materializing param=encoder.block.7.layer.0.SelfAttention.v.weight]\u001b[A\n",
            "Loading weights:  32% 62/196 [00:00<00:00, 4052.91it/s, Materializing param=encoder.block.7.layer.0.layer_norm.weight]     \u001b[A\n",
            "Loading weights:  32% 62/196 [00:00<00:00, 4031.30it/s, Materializing param=encoder.block.7.layer.0.layer_norm.weight]\u001b[A\n",
            "Loading weights:  32% 63/196 [00:00<00:00, 4057.45it/s, Materializing param=encoder.block.7.layer.1.DenseReluDense.wi.weight]\u001b[A\n",
            "Loading weights:  32% 63/196 [00:00<00:00, 4036.06it/s, Materializing param=encoder.block.7.layer.1.DenseReluDense.wi.weight]\u001b[A\n",
            "Loading weights:  33% 64/196 [00:00<00:00, 4061.30it/s, Materializing param=encoder.block.7.layer.1.DenseReluDense.wo.weight]\u001b[A\n",
            "Loading weights:  33% 64/196 [00:00<00:00, 4037.90it/s, Materializing param=encoder.block.7.layer.1.DenseReluDense.wo.weight]\u001b[A\n",
            "Loading weights:  33% 65/196 [00:00<00:00, 4062.61it/s, Materializing param=encoder.block.7.layer.1.layer_norm.weight]       \u001b[A\n",
            "Loading weights:  33% 65/196 [00:00<00:00, 4041.11it/s, Materializing param=encoder.block.7.layer.1.layer_norm.weight]\u001b[A\n",
            "Loading weights:  34% 66/196 [00:00<00:00, 4066.64it/s, Materializing param=encoder.block.8.layer.0.SelfAttention.k.weight]\u001b[A\n",
            "Loading weights:  34% 66/196 [00:00<00:00, 4047.37it/s, Materializing param=encoder.block.8.layer.0.SelfAttention.k.weight]\u001b[A\n",
            "Loading weights:  34% 67/196 [00:00<00:00, 4071.84it/s, Materializing param=encoder.block.8.layer.0.SelfAttention.o.weight]\u001b[A\n",
            "Loading weights:  34% 67/196 [00:00<00:00, 4052.29it/s, Materializing param=encoder.block.8.layer.0.SelfAttention.o.weight]\u001b[A\n",
            "Loading weights:  35% 68/196 [00:00<00:00, 4076.45it/s, Materializing param=encoder.block.8.layer.0.SelfAttention.q.weight]\u001b[A\n",
            "Loading weights:  35% 68/196 [00:00<00:00, 4057.71it/s, Materializing param=encoder.block.8.layer.0.SelfAttention.q.weight]\u001b[A\n",
            "Loading weights:  35% 69/196 [00:00<00:00, 4074.20it/s, Materializing param=encoder.block.8.layer.0.SelfAttention.v.weight]\u001b[A\n",
            "Loading weights:  35% 69/196 [00:00<00:00, 4054.06it/s, Materializing param=encoder.block.8.layer.0.SelfAttention.v.weight]\u001b[A\n",
            "Loading weights:  36% 70/196 [00:00<00:00, 4074.46it/s, Materializing param=encoder.block.8.layer.0.layer_norm.weight]     \u001b[A\n",
            "Loading weights:  36% 70/196 [00:00<00:00, 4055.83it/s, Materializing param=encoder.block.8.layer.0.layer_norm.weight]\u001b[A\n",
            "Loading weights:  36% 71/196 [00:00<00:00, 4079.28it/s, Materializing param=encoder.block.8.layer.1.DenseReluDense.wi.weight]\u001b[A\n",
            "Loading weights:  36% 71/196 [00:00<00:00, 4060.87it/s, Materializing param=encoder.block.8.layer.1.DenseReluDense.wi.weight]\u001b[A\n",
            "Loading weights:  37% 72/196 [00:00<00:00, 4083.59it/s, Materializing param=encoder.block.8.layer.1.DenseReluDense.wo.weight]\u001b[A\n",
            "Loading weights:  37% 72/196 [00:00<00:00, 4065.40it/s, Materializing param=encoder.block.8.layer.1.DenseReluDense.wo.weight]\u001b[A\n",
            "Loading weights:  37% 73/196 [00:00<00:00, 4085.07it/s, Materializing param=encoder.block.8.layer.1.layer_norm.weight]       \u001b[A\n",
            "Loading weights:  37% 73/196 [00:00<00:00, 4066.68it/s, Materializing param=encoder.block.8.layer.1.layer_norm.weight]\u001b[A\n",
            "Loading weights:  38% 74/196 [00:00<00:00, 4088.61it/s, Materializing param=encoder.block.9.layer.0.SelfAttention.k.weight]\u001b[A\n",
            "Loading weights:  38% 74/196 [00:00<00:00, 4070.54it/s, Materializing param=encoder.block.9.layer.0.SelfAttention.k.weight]\u001b[A\n",
            "Loading weights:  38% 75/196 [00:00<00:00, 4091.84it/s, Materializing param=encoder.block.9.layer.0.SelfAttention.o.weight]\u001b[A\n",
            "Loading weights:  38% 75/196 [00:00<00:00, 4074.62it/s, Materializing param=encoder.block.9.layer.0.SelfAttention.o.weight]\u001b[A\n",
            "Loading weights:  39% 76/196 [00:00<00:00, 4095.05it/s, Materializing param=encoder.block.9.layer.0.SelfAttention.q.weight]\u001b[A\n",
            "Loading weights:  39% 76/196 [00:00<00:00, 4078.03it/s, Materializing param=encoder.block.9.layer.0.SelfAttention.q.weight]\u001b[A\n",
            "Loading weights:  39% 77/196 [00:00<00:00, 4096.31it/s, Materializing param=encoder.block.9.layer.0.SelfAttention.v.weight]\u001b[A\n",
            "Loading weights:  39% 77/196 [00:00<00:00, 4078.21it/s, Materializing param=encoder.block.9.layer.0.SelfAttention.v.weight]\u001b[A\n",
            "Loading weights:  40% 78/196 [00:00<00:00, 4093.59it/s, Materializing param=encoder.block.9.layer.0.layer_norm.weight]     \u001b[A\n",
            "Loading weights:  40% 78/196 [00:00<00:00, 4076.20it/s, Materializing param=encoder.block.9.layer.0.layer_norm.weight]\u001b[A\n",
            "Loading weights:  40% 79/196 [00:00<00:00, 4097.01it/s, Materializing param=encoder.block.9.layer.1.DenseReluDense.wi.weight]\u001b[A\n",
            "Loading weights:  40% 79/196 [00:00<00:00, 4080.21it/s, Materializing param=encoder.block.9.layer.1.DenseReluDense.wi.weight]\u001b[A\n",
            "Loading weights:  41% 80/196 [00:00<00:00, 4100.40it/s, Materializing param=encoder.block.9.layer.1.DenseReluDense.wo.weight]\u001b[A\n",
            "Loading weights:  41% 80/196 [00:00<00:00, 4084.23it/s, Materializing param=encoder.block.9.layer.1.DenseReluDense.wo.weight]\u001b[A\n",
            "Loading weights:  41% 81/196 [00:00<00:00, 4105.21it/s, Materializing param=encoder.block.9.layer.1.layer_norm.weight]       \u001b[A\n",
            "Loading weights:  41% 81/196 [00:00<00:00, 4086.84it/s, Materializing param=encoder.block.9.layer.1.layer_norm.weight]\u001b[A\n",
            "Loading weights:  42% 82/196 [00:00<00:00, 4107.10it/s, Materializing param=encoder.block.10.layer.0.SelfAttention.k.weight]\u001b[A\n",
            "Loading weights:  42% 82/196 [00:00<00:00, 4090.20it/s, Materializing param=encoder.block.10.layer.0.SelfAttention.k.weight]\u001b[A\n",
            "Loading weights:  42% 83/196 [00:00<00:00, 4108.81it/s, Materializing param=encoder.block.10.layer.0.SelfAttention.o.weight]\u001b[A\n",
            "Loading weights:  42% 83/196 [00:00<00:00, 4093.21it/s, Materializing param=encoder.block.10.layer.0.SelfAttention.o.weight]\u001b[A\n",
            "Loading weights:  43% 84/196 [00:00<00:00, 4112.78it/s, Materializing param=encoder.block.10.layer.0.SelfAttention.q.weight]\u001b[A\n",
            "Loading weights:  43% 84/196 [00:00<00:00, 4097.14it/s, Materializing param=encoder.block.10.layer.0.SelfAttention.q.weight]\u001b[A\n",
            "Loading weights:  43% 85/196 [00:00<00:00, 4117.05it/s, Materializing param=encoder.block.10.layer.0.SelfAttention.v.weight]\u001b[A\n",
            "Loading weights:  43% 85/196 [00:00<00:00, 4101.75it/s, Materializing param=encoder.block.10.layer.0.SelfAttention.v.weight]\u001b[A\n",
            "Loading weights:  44% 86/196 [00:00<00:00, 4115.21it/s, Materializing param=encoder.block.10.layer.0.layer_norm.weight]     \u001b[A\n",
            "Loading weights:  44% 86/196 [00:00<00:00, 4099.21it/s, Materializing param=encoder.block.10.layer.0.layer_norm.weight]\u001b[A\n",
            "Loading weights:  44% 87/196 [00:00<00:00, 4116.05it/s, Materializing param=encoder.block.10.layer.1.DenseReluDense.wi.weight]\u001b[A\n",
            "Loading weights:  44% 87/196 [00:00<00:00, 4100.33it/s, Materializing param=encoder.block.10.layer.1.DenseReluDense.wi.weight]\u001b[A\n",
            "Loading weights:  45% 88/196 [00:00<00:00, 4118.67it/s, Materializing param=encoder.block.10.layer.1.DenseReluDense.wo.weight]\u001b[A\n",
            "Loading weights:  45% 88/196 [00:00<00:00, 4103.51it/s, Materializing param=encoder.block.10.layer.1.DenseReluDense.wo.weight]\u001b[A\n",
            "Loading weights:  45% 89/196 [00:00<00:00, 4122.14it/s, Materializing param=encoder.block.10.layer.1.layer_norm.weight]       \u001b[A\n",
            "Loading weights:  45% 89/196 [00:00<00:00, 4107.27it/s, Materializing param=encoder.block.10.layer.1.layer_norm.weight]\u001b[A\n",
            "Loading weights:  46% 90/196 [00:00<00:00, 4123.07it/s, Materializing param=encoder.block.11.layer.0.SelfAttention.k.weight]\u001b[A\n",
            "Loading weights:  46% 90/196 [00:00<00:00, 4108.39it/s, Materializing param=encoder.block.11.layer.0.SelfAttention.k.weight]\u001b[A\n",
            "Loading weights:  46% 91/196 [00:00<00:00, 4124.42it/s, Materializing param=encoder.block.11.layer.0.SelfAttention.o.weight]\u001b[A\n",
            "Loading weights:  46% 91/196 [00:00<00:00, 4109.49it/s, Materializing param=encoder.block.11.layer.0.SelfAttention.o.weight]\u001b[A\n",
            "Loading weights:  47% 92/196 [00:00<00:00, 4126.31it/s, Materializing param=encoder.block.11.layer.0.SelfAttention.q.weight]\u001b[A\n",
            "Loading weights:  47% 92/196 [00:00<00:00, 4111.62it/s, Materializing param=encoder.block.11.layer.0.SelfAttention.q.weight]\u001b[A\n",
            "Loading weights:  47% 93/196 [00:00<00:00, 4129.30it/s, Materializing param=encoder.block.11.layer.0.SelfAttention.v.weight]\u001b[A\n",
            "Loading weights:  47% 93/196 [00:00<00:00, 4115.27it/s, Materializing param=encoder.block.11.layer.0.SelfAttention.v.weight]\u001b[A\n",
            "Loading weights:  48% 94/196 [00:00<00:00, 4133.06it/s, Materializing param=encoder.block.11.layer.0.layer_norm.weight]     \u001b[A\n",
            "Loading weights:  48% 94/196 [00:00<00:00, 4118.08it/s, Materializing param=encoder.block.11.layer.0.layer_norm.weight]\u001b[A\n",
            "Loading weights:  48% 95/196 [00:00<00:00, 4131.98it/s, Materializing param=encoder.block.11.layer.1.DenseReluDense.wi.weight]\u001b[A\n",
            "Loading weights:  48% 95/196 [00:00<00:00, 4115.84it/s, Materializing param=encoder.block.11.layer.1.DenseReluDense.wi.weight]\u001b[A\n",
            "Loading weights:  49% 96/196 [00:00<00:00, 4131.34it/s, Materializing param=encoder.block.11.layer.1.DenseReluDense.wo.weight]\u001b[A\n",
            "Loading weights:  49% 96/196 [00:00<00:00, 4117.02it/s, Materializing param=encoder.block.11.layer.1.DenseReluDense.wo.weight]\u001b[A\n",
            "Loading weights:  49% 97/196 [00:00<00:00, 4133.91it/s, Materializing param=encoder.block.11.layer.1.layer_norm.weight]       \u001b[A\n",
            "Loading weights:  49% 97/196 [00:00<00:00, 4120.10it/s, Materializing param=encoder.block.11.layer.1.layer_norm.weight]\u001b[A\n",
            "Loading weights:  50% 98/196 [00:00<00:00, 4137.14it/s, Materializing param=encoder.block.12.layer.0.SelfAttention.k.weight]\u001b[A\n",
            "Loading weights:  50% 98/196 [00:00<00:00, 4121.34it/s, Materializing param=encoder.block.12.layer.0.SelfAttention.k.weight]\u001b[A\n",
            "Loading weights:  51% 99/196 [00:00<00:00, 4135.49it/s, Materializing param=encoder.block.12.layer.0.SelfAttention.o.weight]\u001b[A\n",
            "Loading weights:  51% 99/196 [00:00<00:00, 4121.90it/s, Materializing param=encoder.block.12.layer.0.SelfAttention.o.weight]\u001b[A\n",
            "Loading weights:  51% 100/196 [00:00<00:00, 4136.48it/s, Materializing param=encoder.block.12.layer.0.SelfAttention.q.weight]\u001b[A\n",
            "Loading weights:  51% 100/196 [00:00<00:00, 4123.06it/s, Materializing param=encoder.block.12.layer.0.SelfAttention.q.weight]\u001b[A\n",
            "Loading weights:  52% 101/196 [00:00<00:00, 4138.94it/s, Materializing param=encoder.block.12.layer.0.SelfAttention.v.weight]\u001b[A\n",
            "Loading weights:  52% 101/196 [00:00<00:00, 4125.52it/s, Materializing param=encoder.block.12.layer.0.SelfAttention.v.weight]\u001b[A\n",
            "Loading weights:  52% 102/196 [00:00<00:00, 4141.48it/s, Materializing param=encoder.block.12.layer.0.layer_norm.weight]     \u001b[A\n",
            "Loading weights:  52% 102/196 [00:00<00:00, 4128.45it/s, Materializing param=encoder.block.12.layer.0.layer_norm.weight]\u001b[A\n",
            "Loading weights:  53% 103/196 [00:00<00:00, 4139.57it/s, Materializing param=encoder.block.12.layer.1.DenseReluDense.wi.weight]\u001b[A\n",
            "Loading weights:  53% 103/196 [00:00<00:00, 4125.49it/s, Materializing param=encoder.block.12.layer.1.DenseReluDense.wi.weight]\u001b[A\n",
            "Loading weights:  53% 104/196 [00:00<00:00, 4136.94it/s, Materializing param=encoder.block.12.layer.1.DenseReluDense.wo.weight]\u001b[A\n",
            "Loading weights:  53% 104/196 [00:00<00:00, 4123.37it/s, Materializing param=encoder.block.12.layer.1.DenseReluDense.wo.weight]\u001b[A\n",
            "Loading weights:  54% 105/196 [00:00<00:00, 4138.07it/s, Materializing param=encoder.block.12.layer.1.layer_norm.weight]       \u001b[A\n",
            "Loading weights:  54% 105/196 [00:00<00:00, 4125.24it/s, Materializing param=encoder.block.12.layer.1.layer_norm.weight]\u001b[A\n",
            "Loading weights:  54% 106/196 [00:00<00:00, 4139.82it/s, Materializing param=encoder.block.13.layer.0.SelfAttention.k.weight]\u001b[A\n",
            "Loading weights:  54% 106/196 [00:00<00:00, 4127.06it/s, Materializing param=encoder.block.13.layer.0.SelfAttention.k.weight]\u001b[A\n",
            "Loading weights:  55% 107/196 [00:00<00:00, 4138.45it/s, Materializing param=encoder.block.13.layer.0.SelfAttention.o.weight]\u001b[A\n",
            "Loading weights:  55% 107/196 [00:00<00:00, 4125.63it/s, Materializing param=encoder.block.13.layer.0.SelfAttention.o.weight]\u001b[A\n",
            "Loading weights:  55% 108/196 [00:00<00:00, 4139.61it/s, Materializing param=encoder.block.13.layer.0.SelfAttention.q.weight]\u001b[A\n",
            "Loading weights:  55% 108/196 [00:00<00:00, 4126.97it/s, Materializing param=encoder.block.13.layer.0.SelfAttention.q.weight]\u001b[A\n",
            "Loading weights:  56% 109/196 [00:00<00:00, 4141.79it/s, Materializing param=encoder.block.13.layer.0.SelfAttention.v.weight]\u001b[A\n",
            "Loading weights:  56% 109/196 [00:00<00:00, 4129.56it/s, Materializing param=encoder.block.13.layer.0.SelfAttention.v.weight]\u001b[A\n",
            "Loading weights:  56% 110/196 [00:00<00:00, 4144.64it/s, Materializing param=encoder.block.13.layer.0.layer_norm.weight]     \u001b[A\n",
            "Loading weights:  56% 110/196 [00:00<00:00, 4132.76it/s, Materializing param=encoder.block.13.layer.0.layer_norm.weight]\u001b[A\n",
            "Loading weights:  57% 111/196 [00:00<00:00, 4145.75it/s, Materializing param=encoder.block.13.layer.1.DenseReluDense.wi.weight]\u001b[A\n",
            "Loading weights:  57% 111/196 [00:00<00:00, 4133.46it/s, Materializing param=encoder.block.13.layer.1.DenseReluDense.wi.weight]\u001b[A\n",
            "Loading weights:  57% 112/196 [00:00<00:00, 4143.91it/s, Materializing param=encoder.block.13.layer.1.DenseReluDense.wo.weight]\u001b[A\n",
            "Loading weights:  57% 112/196 [00:00<00:00, 4130.76it/s, Materializing param=encoder.block.13.layer.1.DenseReluDense.wo.weight]\u001b[A\n",
            "Loading weights:  58% 113/196 [00:00<00:00, 4144.50it/s, Materializing param=encoder.block.13.layer.1.layer_norm.weight]       \u001b[A\n",
            "Loading weights:  58% 113/196 [00:00<00:00, 4132.57it/s, Materializing param=encoder.block.13.layer.1.layer_norm.weight]\u001b[A\n",
            "Loading weights:  58% 114/196 [00:00<00:00, 4147.05it/s, Materializing param=encoder.block.14.layer.0.SelfAttention.k.weight]\u001b[A\n",
            "Loading weights:  58% 114/196 [00:00<00:00, 4135.18it/s, Materializing param=encoder.block.14.layer.0.SelfAttention.k.weight]\u001b[A\n",
            "Loading weights:  59% 115/196 [00:00<00:00, 4148.63it/s, Materializing param=encoder.block.14.layer.0.SelfAttention.o.weight]\u001b[A\n",
            "Loading weights:  59% 115/196 [00:00<00:00, 4135.19it/s, Materializing param=encoder.block.14.layer.0.SelfAttention.o.weight]\u001b[A\n",
            "Loading weights:  59% 116/196 [00:00<00:00, 4148.46it/s, Materializing param=encoder.block.14.layer.0.SelfAttention.q.weight]\u001b[A\n",
            "Loading weights:  59% 116/196 [00:00<00:00, 4135.73it/s, Materializing param=encoder.block.14.layer.0.SelfAttention.q.weight]\u001b[A\n",
            "Loading weights:  60% 117/196 [00:00<00:00, 4148.18it/s, Materializing param=encoder.block.14.layer.0.SelfAttention.v.weight]\u001b[A\n",
            "Loading weights:  60% 117/196 [00:00<00:00, 4136.43it/s, Materializing param=encoder.block.14.layer.0.SelfAttention.v.weight]\u001b[A\n",
            "Loading weights:  60% 118/196 [00:00<00:00, 4150.16it/s, Materializing param=encoder.block.14.layer.0.layer_norm.weight]     \u001b[A\n",
            "Loading weights:  60% 118/196 [00:00<00:00, 4139.02it/s, Materializing param=encoder.block.14.layer.0.layer_norm.weight]\u001b[A\n",
            "Loading weights:  61% 119/196 [00:00<00:00, 4153.23it/s, Materializing param=encoder.block.14.layer.1.DenseReluDense.wi.weight]\u001b[A\n",
            "Loading weights:  61% 119/196 [00:00<00:00, 4141.89it/s, Materializing param=encoder.block.14.layer.1.DenseReluDense.wi.weight]\u001b[A\n",
            "Loading weights:  61% 120/196 [00:00<00:00, 4149.42it/s, Materializing param=encoder.block.14.layer.1.DenseReluDense.wo.weight]\u001b[A\n",
            "Loading weights:  61% 120/196 [00:00<00:00, 4136.97it/s, Materializing param=encoder.block.14.layer.1.DenseReluDense.wo.weight]\u001b[A\n",
            "Loading weights:  62% 121/196 [00:00<00:00, 4146.23it/s, Materializing param=encoder.block.14.layer.1.layer_norm.weight]       \u001b[A\n",
            "Loading weights:  62% 121/196 [00:00<00:00, 4134.64it/s, Materializing param=encoder.block.14.layer.1.layer_norm.weight]\u001b[A\n",
            "Loading weights:  62% 122/196 [00:00<00:00, 4147.49it/s, Materializing param=encoder.block.15.layer.0.SelfAttention.k.weight]\u001b[A\n",
            "Loading weights:  62% 122/196 [00:00<00:00, 4136.16it/s, Materializing param=encoder.block.15.layer.0.SelfAttention.k.weight]\u001b[A\n",
            "Loading weights:  63% 123/196 [00:00<00:00, 4149.14it/s, Materializing param=encoder.block.15.layer.0.SelfAttention.o.weight]\u001b[A\n",
            "Loading weights:  63% 123/196 [00:00<00:00, 4138.25it/s, Materializing param=encoder.block.15.layer.0.SelfAttention.o.weight]\u001b[A\n",
            "Loading weights:  63% 124/196 [00:00<00:00, 4149.07it/s, Materializing param=encoder.block.15.layer.0.SelfAttention.q.weight]\u001b[A\n",
            "Loading weights:  63% 124/196 [00:00<00:00, 4137.78it/s, Materializing param=encoder.block.15.layer.0.SelfAttention.q.weight]\u001b[A\n",
            "Loading weights:  64% 125/196 [00:00<00:00, 4149.95it/s, Materializing param=encoder.block.15.layer.0.SelfAttention.v.weight]\u001b[A\n",
            "Loading weights:  64% 125/196 [00:00<00:00, 4138.88it/s, Materializing param=encoder.block.15.layer.0.SelfAttention.v.weight]\u001b[A\n",
            "Loading weights:  64% 126/196 [00:00<00:00, 4151.93it/s, Materializing param=encoder.block.15.layer.0.layer_norm.weight]     \u001b[A\n",
            "Loading weights:  64% 126/196 [00:00<00:00, 4141.32it/s, Materializing param=encoder.block.15.layer.0.layer_norm.weight]\u001b[A\n",
            "Loading weights:  65% 127/196 [00:00<00:00, 1107.92it/s, Materializing param=encoder.block.15.layer.1.DenseReluDense.wi.weight]\u001b[A\n",
            "Loading weights:  65% 127/196 [00:00<00:00, 1106.79it/s, Materializing param=encoder.block.15.layer.1.DenseReluDense.wi.weight]\u001b[A\n",
            "Loading weights:  65% 128/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.15.layer.1.DenseReluDense.wi.weight]\u001b[A\n",
            "Loading weights:  65% 128/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.15.layer.1.DenseReluDense.wo.weight]\u001b[A\n",
            "Loading weights:  65% 128/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.15.layer.1.DenseReluDense.wo.weight]\u001b[A\n",
            "Loading weights:  66% 129/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.15.layer.1.layer_norm.weight]       \u001b[A\n",
            "Loading weights:  66% 129/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.15.layer.1.layer_norm.weight]\u001b[A\n",
            "Loading weights:  66% 130/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.16.layer.0.SelfAttention.k.weight]\u001b[A\n",
            "Loading weights:  66% 130/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.16.layer.0.SelfAttention.k.weight]\u001b[A\n",
            "Loading weights:  67% 131/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.16.layer.0.SelfAttention.o.weight]\u001b[A\n",
            "Loading weights:  67% 131/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.16.layer.0.SelfAttention.o.weight]\u001b[A\n",
            "Loading weights:  67% 132/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.16.layer.0.SelfAttention.q.weight]\u001b[A\n",
            "Loading weights:  67% 132/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.16.layer.0.SelfAttention.q.weight]\u001b[A\n",
            "Loading weights:  68% 133/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.16.layer.0.SelfAttention.v.weight]\u001b[A\n",
            "Loading weights:  68% 133/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.16.layer.0.SelfAttention.v.weight]\u001b[A\n",
            "Loading weights:  68% 134/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.16.layer.0.layer_norm.weight]     \u001b[A\n",
            "Loading weights:  68% 134/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.16.layer.0.layer_norm.weight]\u001b[A\n",
            "Loading weights:  69% 135/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.16.layer.1.DenseReluDense.wi.weight]\u001b[A\n",
            "Loading weights:  69% 135/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.16.layer.1.DenseReluDense.wi.weight]\u001b[A\n",
            "Loading weights:  69% 136/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.16.layer.1.DenseReluDense.wo.weight]\u001b[A\n",
            "Loading weights:  69% 136/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.16.layer.1.DenseReluDense.wo.weight]\u001b[A\n",
            "Loading weights:  70% 137/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.16.layer.1.layer_norm.weight]       \u001b[A\n",
            "Loading weights:  70% 137/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.16.layer.1.layer_norm.weight]\u001b[A\n",
            "Loading weights:  70% 138/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.17.layer.0.SelfAttention.k.weight]\u001b[A\n",
            "Loading weights:  70% 138/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.17.layer.0.SelfAttention.k.weight]\u001b[A\n",
            "Loading weights:  71% 139/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.17.layer.0.SelfAttention.o.weight]\u001b[A\n",
            "Loading weights:  71% 139/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.17.layer.0.SelfAttention.o.weight]\u001b[A\n",
            "Loading weights:  71% 140/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.17.layer.0.SelfAttention.q.weight]\u001b[A\n",
            "Loading weights:  71% 140/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.17.layer.0.SelfAttention.q.weight]\u001b[A\n",
            "Loading weights:  72% 141/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.17.layer.0.SelfAttention.v.weight]\u001b[A\n",
            "Loading weights:  72% 141/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.17.layer.0.SelfAttention.v.weight]\u001b[A\n",
            "Loading weights:  72% 142/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.17.layer.0.layer_norm.weight]     \u001b[A\n",
            "Loading weights:  72% 142/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.17.layer.0.layer_norm.weight]\u001b[A\n",
            "Loading weights:  73% 143/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.17.layer.1.DenseReluDense.wi.weight]\u001b[A\n",
            "Loading weights:  73% 143/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.17.layer.1.DenseReluDense.wi.weight]\u001b[A\n",
            "Loading weights:  73% 144/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.17.layer.1.DenseReluDense.wo.weight]\u001b[A\n",
            "Loading weights:  73% 144/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.17.layer.1.DenseReluDense.wo.weight]\u001b[A\n",
            "Loading weights:  74% 145/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.17.layer.1.layer_norm.weight]       \u001b[A\n",
            "Loading weights:  74% 145/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.17.layer.1.layer_norm.weight]\u001b[A\n",
            "Loading weights:  74% 146/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.18.layer.0.SelfAttention.k.weight]\u001b[A\n",
            "Loading weights:  74% 146/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.18.layer.0.SelfAttention.k.weight]\u001b[A\n",
            "Loading weights:  75% 147/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.18.layer.0.SelfAttention.o.weight]\u001b[A\n",
            "Loading weights:  75% 147/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.18.layer.0.SelfAttention.o.weight]\u001b[A\n",
            "Loading weights:  76% 148/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.18.layer.0.SelfAttention.q.weight]\u001b[A\n",
            "Loading weights:  76% 148/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.18.layer.0.SelfAttention.q.weight]\u001b[A\n",
            "Loading weights:  76% 149/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.18.layer.0.SelfAttention.v.weight]\u001b[A\n",
            "Loading weights:  76% 149/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.18.layer.0.SelfAttention.v.weight]\u001b[A\n",
            "Loading weights:  77% 150/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.18.layer.0.layer_norm.weight]     \u001b[A\n",
            "Loading weights:  77% 150/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.18.layer.0.layer_norm.weight]\u001b[A\n",
            "Loading weights:  77% 151/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.18.layer.1.DenseReluDense.wi.weight]\u001b[A\n",
            "Loading weights:  77% 151/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.18.layer.1.DenseReluDense.wi.weight]\u001b[A\n",
            "Loading weights:  78% 152/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.18.layer.1.DenseReluDense.wo.weight]\u001b[A\n",
            "Loading weights:  78% 152/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.18.layer.1.DenseReluDense.wo.weight]\u001b[A\n",
            "Loading weights:  78% 153/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.18.layer.1.layer_norm.weight]       \u001b[A\n",
            "Loading weights:  78% 153/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.18.layer.1.layer_norm.weight]\u001b[A\n",
            "Loading weights:  79% 154/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.19.layer.0.SelfAttention.k.weight]\u001b[A\n",
            "Loading weights:  79% 154/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.19.layer.0.SelfAttention.k.weight]\u001b[A\n",
            "Loading weights:  79% 155/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.19.layer.0.SelfAttention.o.weight]\u001b[A\n",
            "Loading weights:  79% 155/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.19.layer.0.SelfAttention.o.weight]\u001b[A\n",
            "Loading weights:  80% 156/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.19.layer.0.SelfAttention.q.weight]\u001b[A\n",
            "Loading weights:  80% 156/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.19.layer.0.SelfAttention.q.weight]\u001b[A\n",
            "Loading weights:  80% 157/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.19.layer.0.SelfAttention.v.weight]\u001b[A\n",
            "Loading weights:  80% 157/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.19.layer.0.SelfAttention.v.weight]\u001b[A\n",
            "Loading weights:  81% 158/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.19.layer.0.layer_norm.weight]     \u001b[A\n",
            "Loading weights:  81% 158/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.19.layer.0.layer_norm.weight]\u001b[A\n",
            "Loading weights:  81% 159/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.19.layer.1.DenseReluDense.wi.weight]\u001b[A\n",
            "Loading weights:  81% 159/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.19.layer.1.DenseReluDense.wi.weight]\u001b[A\n",
            "Loading weights:  82% 160/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.19.layer.1.DenseReluDense.wo.weight]\u001b[A\n",
            "Loading weights:  82% 160/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.19.layer.1.DenseReluDense.wo.weight]\u001b[A\n",
            "Loading weights:  82% 161/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.19.layer.1.layer_norm.weight]       \u001b[A\n",
            "Loading weights:  82% 161/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.19.layer.1.layer_norm.weight]\u001b[A\n",
            "Loading weights:  83% 162/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.20.layer.0.SelfAttention.k.weight]\u001b[A\n",
            "Loading weights:  83% 162/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.20.layer.0.SelfAttention.k.weight]\u001b[A\n",
            "Loading weights:  83% 163/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.20.layer.0.SelfAttention.o.weight]\u001b[A\n",
            "Loading weights:  83% 163/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.20.layer.0.SelfAttention.o.weight]\u001b[A\n",
            "Loading weights:  84% 164/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.20.layer.0.SelfAttention.q.weight]\u001b[A\n",
            "Loading weights:  84% 164/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.20.layer.0.SelfAttention.q.weight]\u001b[A\n",
            "Loading weights:  84% 165/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.20.layer.0.SelfAttention.v.weight]\u001b[A\n",
            "Loading weights:  84% 165/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.20.layer.0.SelfAttention.v.weight]\u001b[A\n",
            "Loading weights:  85% 166/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.20.layer.0.layer_norm.weight]     \u001b[A\n",
            "Loading weights:  85% 166/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.20.layer.0.layer_norm.weight]\u001b[A\n",
            "Loading weights:  85% 167/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.20.layer.1.DenseReluDense.wi.weight]\u001b[A\n",
            "Loading weights:  85% 167/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.20.layer.1.DenseReluDense.wi.weight]\u001b[A\n",
            "Loading weights:  86% 168/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.20.layer.1.DenseReluDense.wo.weight]\u001b[A\n",
            "Loading weights:  86% 168/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.20.layer.1.DenseReluDense.wo.weight]\u001b[A\n",
            "Loading weights:  86% 169/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.20.layer.1.layer_norm.weight]       \u001b[A\n",
            "Loading weights:  86% 169/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.20.layer.1.layer_norm.weight]\u001b[A\n",
            "Loading weights:  87% 170/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.21.layer.0.SelfAttention.k.weight]\u001b[A\n",
            "Loading weights:  87% 170/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.21.layer.0.SelfAttention.k.weight]\u001b[A\n",
            "Loading weights:  87% 171/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.21.layer.0.SelfAttention.o.weight]\u001b[A\n",
            "Loading weights:  87% 171/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.21.layer.0.SelfAttention.o.weight]\u001b[A\n",
            "Loading weights:  88% 172/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.21.layer.0.SelfAttention.q.weight]\u001b[A\n",
            "Loading weights:  88% 172/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.21.layer.0.SelfAttention.q.weight]\u001b[A\n",
            "Loading weights:  88% 173/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.21.layer.0.SelfAttention.v.weight]\u001b[A\n",
            "Loading weights:  88% 173/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.21.layer.0.SelfAttention.v.weight]\u001b[A\n",
            "Loading weights:  89% 174/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.21.layer.0.layer_norm.weight]     \u001b[A\n",
            "Loading weights:  89% 174/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.21.layer.0.layer_norm.weight]\u001b[A\n",
            "Loading weights:  89% 175/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.21.layer.1.DenseReluDense.wi.weight]\u001b[A\n",
            "Loading weights:  89% 175/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.21.layer.1.DenseReluDense.wi.weight]\u001b[A\n",
            "Loading weights:  90% 176/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.21.layer.1.DenseReluDense.wo.weight]\u001b[A\n",
            "Loading weights:  90% 176/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.21.layer.1.DenseReluDense.wo.weight]\u001b[A\n",
            "Loading weights:  90% 177/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.21.layer.1.layer_norm.weight]       \u001b[A\n",
            "Loading weights:  90% 177/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.21.layer.1.layer_norm.weight]\u001b[A\n",
            "Loading weights:  91% 178/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.22.layer.0.SelfAttention.k.weight]\u001b[A\n",
            "Loading weights:  91% 178/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.22.layer.0.SelfAttention.k.weight]\u001b[A\n",
            "Loading weights:  91% 179/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.22.layer.0.SelfAttention.o.weight]\u001b[A\n",
            "Loading weights:  91% 179/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.22.layer.0.SelfAttention.o.weight]\u001b[A\n",
            "Loading weights:  92% 180/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.22.layer.0.SelfAttention.q.weight]\u001b[A\n",
            "Loading weights:  92% 180/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.22.layer.0.SelfAttention.q.weight]\u001b[A\n",
            "Loading weights:  92% 181/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.22.layer.0.SelfAttention.v.weight]\u001b[A\n",
            "Loading weights:  92% 181/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.22.layer.0.SelfAttention.v.weight]\u001b[A\n",
            "Loading weights:  93% 182/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.22.layer.0.layer_norm.weight]     \u001b[A\n",
            "Loading weights:  93% 182/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.22.layer.0.layer_norm.weight]\u001b[A\n",
            "Loading weights:  93% 183/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.22.layer.1.DenseReluDense.wi.weight]\u001b[A\n",
            "Loading weights:  93% 183/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.22.layer.1.DenseReluDense.wi.weight]\u001b[A\n",
            "Loading weights:  94% 184/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.22.layer.1.DenseReluDense.wo.weight]\u001b[A\n",
            "Loading weights:  94% 184/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.22.layer.1.DenseReluDense.wo.weight]\u001b[A\n",
            "Loading weights:  94% 185/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.22.layer.1.layer_norm.weight]       \u001b[A\n",
            "Loading weights:  94% 185/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.22.layer.1.layer_norm.weight]\u001b[A\n",
            "Loading weights:  95% 186/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.23.layer.0.SelfAttention.k.weight]\u001b[A\n",
            "Loading weights:  95% 186/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.23.layer.0.SelfAttention.k.weight]\u001b[A\n",
            "Loading weights:  95% 187/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.23.layer.0.SelfAttention.o.weight]\u001b[A\n",
            "Loading weights:  95% 187/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.23.layer.0.SelfAttention.o.weight]\u001b[A\n",
            "Loading weights:  96% 188/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.23.layer.0.SelfAttention.q.weight]\u001b[A\n",
            "Loading weights:  96% 188/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.23.layer.0.SelfAttention.q.weight]\u001b[A\n",
            "Loading weights:  96% 189/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.23.layer.0.SelfAttention.v.weight]\u001b[A\n",
            "Loading weights:  96% 189/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.23.layer.0.SelfAttention.v.weight]\u001b[A\n",
            "Loading weights:  97% 190/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.23.layer.0.layer_norm.weight]     \u001b[A\n",
            "Loading weights:  97% 190/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.23.layer.0.layer_norm.weight]\u001b[A\n",
            "Loading weights:  97% 191/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.23.layer.1.DenseReluDense.wi.weight]\u001b[A\n",
            "Loading weights:  97% 191/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.23.layer.1.DenseReluDense.wi.weight]\u001b[A\n",
            "Loading weights:  98% 192/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.23.layer.1.DenseReluDense.wo.weight]\u001b[A\n",
            "Loading weights:  98% 192/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.23.layer.1.DenseReluDense.wo.weight]\u001b[A\n",
            "Loading weights:  98% 193/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.23.layer.1.layer_norm.weight]       \u001b[A\n",
            "Loading weights:  98% 193/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.block.23.layer.1.layer_norm.weight]\u001b[A\n",
            "Loading weights:  99% 194/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.embed_tokens.weight]               \u001b[A\n",
            "Loading weights:  99% 194/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.embed_tokens.weight]\u001b[A\n",
            "Loading weights:  99% 195/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.final_layer_norm.weight]\u001b[A\n",
            "Loading weights:  99% 195/196 [00:00<00:00, 1113.35it/s, Materializing param=encoder.final_layer_norm.weight]\u001b[A\n",
            "Loading weights: 100% 196/196 [00:00<00:00, 1113.35it/s, Materializing param=shared.weight]                  \u001b[A\n",
            "Loading weights: 100% 196/196 [00:00<00:00, 903.26it/s, Materializing param=shared.weight] \n",
            "The tied weights mapping and config for this model specifies to tie shared.weight to encoder.embed_tokens.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "\u001b[1mT5EncoderModel LOAD REPORT\u001b[0m from: Rostlab/prot_t5_xl_uniref50\n",
            "Key            | Status     |  | \n",
            "---------------+------------+--+-\n",
            "lm_head.weight | \u001b[38;5;208mUNEXPECTED\u001b[0m |  | \n",
            "\n",
            "\u001b[3mNotes:\n",
            "- \u001b[38;5;208mUNEXPECTED\u001b[0m\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n",
            "model.safetensors:  64% 7.19G/11.3G [01:58<01:32, 44.2MB/s]âœ… FP16 mode enabled (GPU)\n",
            "âœ… Model ready. Embedding dim: 1024\n",
            "\n",
            "============================================================\n",
            "ðŸš€ ProtT5-based Preprocessing Pipeline\n",
            "============================================================\n",
            "ðŸ“„ all.train.fasta: 5680 records\n",
            "ðŸ“„ all.test.fasta: 1535 records\n",
            "\n",
            "ðŸ·ï¸ Label distribution:\n",
            "label\n",
            "allergen    3665\n",
            "protein     3550\n",
            "Name: count, dtype: int64\n",
            "\n",
            "ðŸ“Š Splits â†’ Train: 5049, Val: 1083, Test: 1083\n",
            "\n",
            "ðŸ”„ Embedding Train (5049 seqs)...\n",
            "model.safetensors: 100% 11.3G/11.3G [03:09<00:00, 59.6MB/s]\n",
            "âœ… Train â†’ X:(5049, 112, 1024), y:(5049,)\n",
            "2026-02-15 15:26:02.720545: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "I0000 00:00:1771169162.726828    1527 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 11203 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "2026-02-15 15:26:02.789400: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 2316238848 exceeds 10% of free system memory.\n",
            "2026-02-15 15:26:05.972195: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 2316238848 exceeds 10% of free system memory.\n",
            "\n",
            "ðŸ”„ Embedding Validation (1083 seqs)...\n",
            "âœ… Validation â†’ X:(1083, 112, 1024), y:(1083,)\n",
            "2026-02-15 15:26:46.967257: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 496828416 exceeds 10% of free system memory.\n",
            "\n",
            "ðŸ”„ Embedding Test (1083 seqs)...\n",
            "âœ… Test â†’ X:(1083, 112, 1024), y:(1083,)\n",
            "2026-02-15 15:27:25.801783: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 496828416 exceeds 10% of free system memory.\n",
            "2026-02-15 15:27:26.585262: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 496828416 exceeds 10% of free system memory.\n",
            "WARNING:tensorflow:From /content/datapreprocess.py:198: save (from tensorflow.python.data.experimental.ops.io) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.save(...)` instead.\n",
            "ðŸ’¾ Saved: preprocessed_data/train_dataset\n",
            "ðŸ’¾ Saved: preprocessed_data/val_dataset\n",
            "ðŸ’¾ Saved: preprocessed_data/test_dataset\n",
            "\n",
            "âœ… All done!\n",
            "ðŸ“ Output dir: /content/preprocessed_data\n",
            "ðŸŽ¯ Model input shape: (None, 112, 1024)\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "with open(\"train.py\", \"r\") as f:\n",
        "    content = f.read()\n",
        "\n",
        "content = content.replace(\"import tensorflow_addons as tfa\", \"# import tensorflow_addons as tfa\")\n",
        "\n",
        "content = content.replace(\"tfa.optimizers.AdamW\", \"tf.keras.optimizers.AdamW\")\n",
        "\n",
        "with open(\"train.py\", \"w\") as f:\n",
        "    f.write(content)\n",
        "\n",
        "print(\"âœ… train.py has been patched to run without tensorflow-addons!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ubAki49C7pY2",
        "outputId": "2679d32f-b7fa-4377-930d-18342c4b0817"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… train.py has been patched to run without tensorflow-addons!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "with open(\"model.py\", \"r\") as f:\n",
        "    content = f.read()\n",
        "\n",
        "# FIX: Change \"out_dim / ratio\" to \"int(out_dim / ratio)\"\n",
        "# This forces the number of neurons to be an integer (e.g., 64 instead of 64.0)\n",
        "content = content.replace(\"units=out_dim / ratio\", \"units=int(out_dim / ratio)\")\n",
        "\n",
        "with open(\"model.py\", \"w\") as f:\n",
        "    f.write(content)\n",
        "\n",
        "print(\"âœ… model.py has been patched to fix the Float/Integer error!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uKmHKzh58L3L",
        "outputId": "de9aa6c6-7b43-4af1-e09e-ba154846cf95"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… model.py has been patched to fix the Float/Integer error!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Read the current model.py (which already has the previous float/int fix)\n",
        "with open(\"model.py\", \"r\") as f:\n",
        "    content = f.read()\n",
        "\n",
        "# FIX: Change the signature to allow optional training argument\n",
        "content = content.replace(\"def call(self, inputs, training):\", \"def call(self, inputs, training=None):\")\n",
        "\n",
        "# Save the fixed file\n",
        "with open(\"model.py\", \"w\") as f:\n",
        "    f.write(content)\n",
        "\n",
        "print(\"âœ… model.py has been patched to fix the TransformerBlock error!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n86AQ6_r8cwa",
        "outputId": "429c7507-69d5-4265-8b6b-e3c2ef77744e"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… model.py has been patched to fix the TransformerBlock error!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "with open(\"model.py\", \"r\") as f:\n",
        "    content = f.read()\n",
        "\n",
        "# FIX: In Keras 3, 'alpha' was renamed to 'negative_slope' for relu\n",
        "# We replace the specific function call used in the MLP section\n",
        "content = content.replace(\"tf.keras.activations.relu(x, alpha=0.1)\", \"tf.keras.activations.relu(x, negative_slope=0.1)\")\n",
        "\n",
        "# Save the fixed file\n",
        "with open(\"model.py\", \"w\") as f:\n",
        "    f.write(content)\n",
        "\n",
        "print(\"âœ… model.py has been patched for Keras 3 compatibility!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ag0fM4Ds8xrP",
        "outputId": "252831aa-4fb9-4858-af01-0e42e5ab0eb0"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… model.py has been patched for Keras 3 compatibility!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KfcOF0vn4V2j",
        "outputId": "018ac7e6-0276-456c-f755-d451d1d94d5b"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2026-02-15 16:50:06.670861: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1771174206.702331   27358 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1771174206.712805   27358 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1771174206.736170   27358 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1771174206.736209   27358 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1771174206.736217   27358 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1771174206.736224   27358 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2026-02-15 16:50:06.743136: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2026-02-15 16:50:13.290395: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "I0000 00:00:1771174213.291850   27358 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13757 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "ðŸ–¥ï¸  Replicas: 1\n",
            "E0215 16:50:13.334799324   27358 server_chttp2.cc:40]        {\"created\":\"@1771174213.334636637\",\"description\":\"No address added out of total 1 resolved\",\"file\":\"external/com_github_grpc_grpc/src/core/ext/transport/chttp2/server/chttp2_server.cc\",\"file_line\":395,\"referenced_errors\":[{\"created\":\"@1771174213.334633139\",\"description\":\"Failed to add any wildcard listeners\",\"file\":\"external/com_github_grpc_grpc/src/core/lib/iomgr/tcp_server_posix.cc\",\"file_line\":342,\"referenced_errors\":[{\"created\":\"@1771174213.334616543\",\"description\":\"Unable to configure socket\",\"fd\":38,\"file\":\"external/com_github_grpc_grpc/src/core/lib/iomgr/tcp_server_utils_posix_common.cc\",\"file_line\":216,\"referenced_errors\":[{\"created\":\"@1771174213.334611510\",\"description\":\"Address already in use\",\"errno\":98,\"file\":\"external/com_github_grpc_grpc/src/core/lib/iomgr/tcp_server_utils_posix_common.cc\",\"file_line\":189,\"os_error\":\"Address already in use\",\"syscall\":\"bind\"}]},{\"created\":\"@1771174213.334632569\",\"description\":\"Unable to configure socket\",\"fd\":38,\"file\":\"external/com_github_grpc_grpc/src/core/lib/iomgr/tcp_server_utils_posix_common.cc\",\"file_line\":216,\"referenced_errors\":[{\"created\":\"@1771174213.334629610\",\"description\":\"Address already in use\",\"errno\":98,\"file\":\"external/com_github_grpc_grpc/src/core/lib/iomgr/tcp_server_utils_posix_common.cc\",\"file_line\":189,\"os_error\":\"Address already in use\",\"syscall\":\"bind\"}]}]}]}\n",
            "2026-02-15 16:50:13.334899: E external/local_xla/xla/tsl/profiler/rpc/profiler_server.cc:43] Unable to bind to [::]:6000 selected port:0\n",
            "\n",
            "ðŸ“‚ Dataset'ler yÃ¼kleniyor...\n",
            "WARNING:tensorflow:From /content/train.py:32: load (from tensorflow.python.data.experimental.ops.io) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.load(...)` instead.\n",
            "âœ… Dataset'ler yÃ¼klendi\n",
            "ðŸŽ¯ Batch size: 8\n",
            "âœ… Dataset pipeline hazÄ±r\n",
            "\n",
            "ðŸ—ï¸  Model oluÅŸturuluyor...\n",
            "âœ… Model compile edildi\n",
            "\n",
            "\u001b[1mModel: \"functional_8\"\u001b[0m\n",
            "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
            "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
            "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
            "â”‚ input_layer         â”‚ (\u001b[96mNone\u001b[0m, \u001b[32m112\u001b[0m, \u001b[32m1024\u001b[0m) â”‚          \u001b[32m0\u001b[0m â”‚ -                 â”‚\n",
            "â”‚ (\u001b[94mInputLayer\u001b[0m)        â”‚                   â”‚            â”‚                   â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ conv1d (\u001b[94mConv1D\u001b[0m)     â”‚ (\u001b[96mNone\u001b[0m, \u001b[32m112\u001b[0m, \u001b[32m128\u001b[0m)  â”‚  \u001b[32m1,441,920\u001b[0m â”‚ input_layer[\u001b[32m0\u001b[0m][\u001b[32m0\u001b[0m] â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ ecb_1__0            â”‚ (\u001b[96mNone\u001b[0m, \u001b[32m112\u001b[0m, \u001b[32m128\u001b[0m)  â”‚     \u001b[32m16,576\u001b[0m â”‚ conv1d[\u001b[32m0\u001b[0m][\u001b[32m0\u001b[0m]      â”‚\n",
            "â”‚ (\u001b[94mSqueezeExcitationâ€¦\u001b[0m â”‚                   â”‚            â”‚                   â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ batch_normalization â”‚ (\u001b[96mNone\u001b[0m, \u001b[32m112\u001b[0m, \u001b[32m128\u001b[0m)  â”‚        \u001b[32m512\u001b[0m â”‚ ecb_1__0[\u001b[32m0\u001b[0m][\u001b[32m0\u001b[0m]    â”‚\n",
            "â”‚ (\u001b[94mBatchNormalizatioâ€¦\u001b[0m â”‚                   â”‚            â”‚                   â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ conv1d_1 (\u001b[94mConv1D\u001b[0m)   â”‚ (\u001b[96mNone\u001b[0m, \u001b[32m112\u001b[0m, \u001b[32m128\u001b[0m)  â”‚    \u001b[32m180,352\u001b[0m â”‚ batch_normalizatâ€¦ â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ ecb_1__1            â”‚ (\u001b[96mNone\u001b[0m, \u001b[32m112\u001b[0m, \u001b[32m128\u001b[0m)  â”‚     \u001b[32m16,576\u001b[0m â”‚ conv1d_1[\u001b[32m0\u001b[0m][\u001b[32m0\u001b[0m]    â”‚\n",
            "â”‚ (\u001b[94mSqueezeExcitationâ€¦\u001b[0m â”‚                   â”‚            â”‚                   â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ batch_normalizatioâ€¦ â”‚ (\u001b[96mNone\u001b[0m, \u001b[32m112\u001b[0m, \u001b[32m128\u001b[0m)  â”‚        \u001b[32m512\u001b[0m â”‚ ecb_1__1[\u001b[32m0\u001b[0m][\u001b[32m0\u001b[0m]    â”‚\n",
            "â”‚ (\u001b[94mBatchNormalizatioâ€¦\u001b[0m â”‚                   â”‚            â”‚                   â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ dropout (\u001b[94mDropout\u001b[0m)   â”‚ (\u001b[96mNone\u001b[0m, \u001b[32m112\u001b[0m, \u001b[32m128\u001b[0m)  â”‚          \u001b[32m0\u001b[0m â”‚ batch_normalizatâ€¦ â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ conv1d_2 (\u001b[94mConv1D\u001b[0m)   â”‚ (\u001b[96mNone\u001b[0m, \u001b[32m56\u001b[0m, \u001b[32m128\u001b[0m)   â”‚    \u001b[32m180,352\u001b[0m â”‚ dropout[\u001b[32m0\u001b[0m][\u001b[32m0\u001b[0m]     â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ conv1d_3 (\u001b[94mConv1D\u001b[0m)   â”‚ (\u001b[96mNone\u001b[0m, \u001b[32m56\u001b[0m, \u001b[32m128\u001b[0m)   â”‚    \u001b[32m180,352\u001b[0m â”‚ conv1d_2[\u001b[32m0\u001b[0m][\u001b[32m0\u001b[0m]    â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ ecb_2__0            â”‚ (\u001b[96mNone\u001b[0m, \u001b[32m56\u001b[0m, \u001b[32m128\u001b[0m)   â”‚     \u001b[32m16,576\u001b[0m â”‚ conv1d_3[\u001b[32m0\u001b[0m][\u001b[32m0\u001b[0m]    â”‚\n",
            "â”‚ (\u001b[94mSqueezeExcitationâ€¦\u001b[0m â”‚                   â”‚            â”‚                   â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ batch_normalizatioâ€¦ â”‚ (\u001b[96mNone\u001b[0m, \u001b[32m56\u001b[0m, \u001b[32m128\u001b[0m)   â”‚        \u001b[32m512\u001b[0m â”‚ ecb_2__0[\u001b[32m0\u001b[0m][\u001b[32m0\u001b[0m]    â”‚\n",
            "â”‚ (\u001b[94mBatchNormalizatioâ€¦\u001b[0m â”‚                   â”‚            â”‚                   â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ conv1d_4 (\u001b[94mConv1D\u001b[0m)   â”‚ (\u001b[96mNone\u001b[0m, \u001b[32m56\u001b[0m, \u001b[32m128\u001b[0m)   â”‚    \u001b[32m180,352\u001b[0m â”‚ batch_normalizatâ€¦ â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ ecb_2__1            â”‚ (\u001b[96mNone\u001b[0m, \u001b[32m56\u001b[0m, \u001b[32m128\u001b[0m)   â”‚     \u001b[32m16,576\u001b[0m â”‚ conv1d_4[\u001b[32m0\u001b[0m][\u001b[32m0\u001b[0m]    â”‚\n",
            "â”‚ (\u001b[94mSqueezeExcitationâ€¦\u001b[0m â”‚                   â”‚            â”‚                   â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ batch_normalizatioâ€¦ â”‚ (\u001b[96mNone\u001b[0m, \u001b[32m56\u001b[0m, \u001b[32m128\u001b[0m)   â”‚        \u001b[32m512\u001b[0m â”‚ ecb_2__1[\u001b[32m0\u001b[0m][\u001b[32m0\u001b[0m]    â”‚\n",
            "â”‚ (\u001b[94mBatchNormalizatioâ€¦\u001b[0m â”‚                   â”‚            â”‚                   â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ dropout_1 (\u001b[94mDropout\u001b[0m) â”‚ (\u001b[96mNone\u001b[0m, \u001b[32m56\u001b[0m, \u001b[32m128\u001b[0m)   â”‚          \u001b[32m0\u001b[0m â”‚ batch_normalizatâ€¦ â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ conv1d_5 (\u001b[94mConv1D\u001b[0m)   â”‚ (\u001b[96mNone\u001b[0m, \u001b[32m28\u001b[0m, \u001b[32m128\u001b[0m)   â”‚    \u001b[32m180,352\u001b[0m â”‚ dropout_1[\u001b[32m0\u001b[0m][\u001b[32m0\u001b[0m]   â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ conv1d_6 (\u001b[94mConv1D\u001b[0m)   â”‚ (\u001b[96mNone\u001b[0m, \u001b[32m28\u001b[0m, \u001b[32m256\u001b[0m)   â”‚    \u001b[32m360,704\u001b[0m â”‚ conv1d_5[\u001b[32m0\u001b[0m][\u001b[32m0\u001b[0m]    â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ ecb_3__0            â”‚ (\u001b[96mNone\u001b[0m, \u001b[32m28\u001b[0m, \u001b[32m256\u001b[0m)   â”‚     \u001b[32m65,920\u001b[0m â”‚ conv1d_6[\u001b[32m0\u001b[0m][\u001b[32m0\u001b[0m]    â”‚\n",
            "â”‚ (\u001b[94mSqueezeExcitationâ€¦\u001b[0m â”‚                   â”‚            â”‚                   â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ batch_normalizatioâ€¦ â”‚ (\u001b[96mNone\u001b[0m, \u001b[32m28\u001b[0m, \u001b[32m256\u001b[0m)   â”‚      \u001b[32m1,024\u001b[0m â”‚ ecb_3__0[\u001b[32m0\u001b[0m][\u001b[32m0\u001b[0m]    â”‚\n",
            "â”‚ (\u001b[94mBatchNormalizatioâ€¦\u001b[0m â”‚                   â”‚            â”‚                   â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ conv1d_7 (\u001b[94mConv1D\u001b[0m)   â”‚ (\u001b[96mNone\u001b[0m, \u001b[32m28\u001b[0m, \u001b[32m256\u001b[0m)   â”‚    \u001b[32m721,152\u001b[0m â”‚ batch_normalizatâ€¦ â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ ecb_3__1            â”‚ (\u001b[96mNone\u001b[0m, \u001b[32m28\u001b[0m, \u001b[32m256\u001b[0m)   â”‚     \u001b[32m65,920\u001b[0m â”‚ conv1d_7[\u001b[32m0\u001b[0m][\u001b[32m0\u001b[0m]    â”‚\n",
            "â”‚ (\u001b[94mSqueezeExcitationâ€¦\u001b[0m â”‚                   â”‚            â”‚                   â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ batch_normalizatioâ€¦ â”‚ (\u001b[96mNone\u001b[0m, \u001b[32m28\u001b[0m, \u001b[32m256\u001b[0m)   â”‚      \u001b[32m1,024\u001b[0m â”‚ ecb_3__1[\u001b[32m0\u001b[0m][\u001b[32m0\u001b[0m]    â”‚\n",
            "â”‚ (\u001b[94mBatchNormalizatioâ€¦\u001b[0m â”‚                   â”‚            â”‚                   â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ dropout_2 (\u001b[94mDropout\u001b[0m) â”‚ (\u001b[96mNone\u001b[0m, \u001b[32m28\u001b[0m, \u001b[32m256\u001b[0m)   â”‚          \u001b[32m0\u001b[0m â”‚ batch_normalizatâ€¦ â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ conv1d_8 (\u001b[94mConv1D\u001b[0m)   â”‚ (\u001b[96mNone\u001b[0m, \u001b[32m14\u001b[0m, \u001b[32m256\u001b[0m)   â”‚    \u001b[32m721,152\u001b[0m â”‚ dropout_2[\u001b[32m0\u001b[0m][\u001b[32m0\u001b[0m]   â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ conv1d_9 (\u001b[94mConv1D\u001b[0m)   â”‚ (\u001b[96mNone\u001b[0m, \u001b[32m14\u001b[0m, \u001b[32m256\u001b[0m)   â”‚    \u001b[32m721,152\u001b[0m â”‚ conv1d_8[\u001b[32m0\u001b[0m][\u001b[32m0\u001b[0m]    â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ ecb_4__0            â”‚ (\u001b[96mNone\u001b[0m, \u001b[32m14\u001b[0m, \u001b[32m256\u001b[0m)   â”‚     \u001b[32m65,920\u001b[0m â”‚ conv1d_9[\u001b[32m0\u001b[0m][\u001b[32m0\u001b[0m]    â”‚\n",
            "â”‚ (\u001b[94mSqueezeExcitationâ€¦\u001b[0m â”‚                   â”‚            â”‚                   â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ batch_normalizatioâ€¦ â”‚ (\u001b[96mNone\u001b[0m, \u001b[32m14\u001b[0m, \u001b[32m256\u001b[0m)   â”‚      \u001b[32m1,024\u001b[0m â”‚ ecb_4__0[\u001b[32m0\u001b[0m][\u001b[32m0\u001b[0m]    â”‚\n",
            "â”‚ (\u001b[94mBatchNormalizatioâ€¦\u001b[0m â”‚                   â”‚            â”‚                   â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ conv1d_10 (\u001b[94mConv1D\u001b[0m)  â”‚ (\u001b[96mNone\u001b[0m, \u001b[32m14\u001b[0m, \u001b[32m256\u001b[0m)   â”‚    \u001b[32m721,152\u001b[0m â”‚ batch_normalizatâ€¦ â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ ecb_4__1            â”‚ (\u001b[96mNone\u001b[0m, \u001b[32m14\u001b[0m, \u001b[32m256\u001b[0m)   â”‚     \u001b[32m65,920\u001b[0m â”‚ conv1d_10[\u001b[32m0\u001b[0m][\u001b[32m0\u001b[0m]   â”‚\n",
            "â”‚ (\u001b[94mSqueezeExcitationâ€¦\u001b[0m â”‚                   â”‚            â”‚                   â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ batch_normalizatioâ€¦ â”‚ (\u001b[96mNone\u001b[0m, \u001b[32m14\u001b[0m, \u001b[32m256\u001b[0m)   â”‚      \u001b[32m1,024\u001b[0m â”‚ ecb_4__1[\u001b[32m0\u001b[0m][\u001b[32m0\u001b[0m]    â”‚\n",
            "â”‚ (\u001b[94mBatchNormalizatioâ€¦\u001b[0m â”‚                   â”‚            â”‚                   â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ dropout_3 (\u001b[94mDropout\u001b[0m) â”‚ (\u001b[96mNone\u001b[0m, \u001b[32m14\u001b[0m, \u001b[32m256\u001b[0m)   â”‚          \u001b[32m0\u001b[0m â”‚ batch_normalizatâ€¦ â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ position_embedding  â”‚ (\u001b[96mNone\u001b[0m, \u001b[32m14\u001b[0m, \u001b[32m256\u001b[0m)   â”‚          \u001b[32m0\u001b[0m â”‚ dropout_3[\u001b[32m0\u001b[0m][\u001b[32m0\u001b[0m]   â”‚\n",
            "â”‚ (\u001b[94mPositionEmbedding\u001b[0m) â”‚                   â”‚            â”‚                   â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ transformer_block   â”‚ (\u001b[96mNone\u001b[0m, \u001b[32m14\u001b[0m, \u001b[32m256\u001b[0m)   â”‚  \u001b[32m2,170,496\u001b[0m â”‚ position_embeddiâ€¦ â”‚\n",
            "â”‚ (\u001b[94mTransformerBlock\u001b[0m)  â”‚                   â”‚            â”‚                   â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ transformer_block_2 â”‚ (\u001b[96mNone\u001b[0m, \u001b[32m14\u001b[0m, \u001b[32m256\u001b[0m)   â”‚  \u001b[32m2,170,496\u001b[0m â”‚ transformer_blocâ€¦ â”‚\n",
            "â”‚ (\u001b[94mTransformerBlock\u001b[0m)  â”‚                   â”‚            â”‚                   â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ transformer_block_1 â”‚ (\u001b[96mNone\u001b[0m, \u001b[32m14\u001b[0m, \u001b[32m256\u001b[0m)   â”‚  \u001b[32m2,170,496\u001b[0m â”‚ transformer_blocâ€¦ â”‚\n",
            "â”‚ (\u001b[94mTransformerBlock\u001b[0m)  â”‚                   â”‚            â”‚                   â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ add (\u001b[94mAdd\u001b[0m)           â”‚ (\u001b[96mNone\u001b[0m, \u001b[32m14\u001b[0m, \u001b[32m256\u001b[0m)   â”‚          \u001b[32m0\u001b[0m â”‚ transformer_blocâ€¦ â”‚\n",
            "â”‚                     â”‚                   â”‚            â”‚ transformer_blocâ€¦ â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ transformer_block_4 â”‚ (\u001b[96mNone\u001b[0m, \u001b[32m14\u001b[0m, \u001b[32m256\u001b[0m)   â”‚  \u001b[32m2,170,496\u001b[0m â”‚ add[\u001b[32m0\u001b[0m][\u001b[32m0\u001b[0m]         â”‚\n",
            "â”‚ (\u001b[94mTransformerBlock\u001b[0m)  â”‚                   â”‚            â”‚                   â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ transformer_block_3 â”‚ (\u001b[96mNone\u001b[0m, \u001b[32m14\u001b[0m, \u001b[32m256\u001b[0m)   â”‚  \u001b[32m2,170,496\u001b[0m â”‚ add[\u001b[32m0\u001b[0m][\u001b[32m0\u001b[0m]         â”‚\n",
            "â”‚ (\u001b[94mTransformerBlock\u001b[0m)  â”‚                   â”‚            â”‚                   â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ add_1 (\u001b[94mAdd\u001b[0m)         â”‚ (\u001b[96mNone\u001b[0m, \u001b[32m14\u001b[0m, \u001b[32m256\u001b[0m)   â”‚          \u001b[32m0\u001b[0m â”‚ transformer_blocâ€¦ â”‚\n",
            "â”‚                     â”‚                   â”‚            â”‚ transformer_blocâ€¦ â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ transformer_block_6 â”‚ (\u001b[96mNone\u001b[0m, \u001b[32m14\u001b[0m, \u001b[32m256\u001b[0m)   â”‚  \u001b[32m2,170,496\u001b[0m â”‚ add_1[\u001b[32m0\u001b[0m][\u001b[32m0\u001b[0m]       â”‚\n",
            "â”‚ (\u001b[94mTransformerBlock\u001b[0m)  â”‚                   â”‚            â”‚                   â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ transformer_block_5 â”‚ (\u001b[96mNone\u001b[0m, \u001b[32m14\u001b[0m, \u001b[32m256\u001b[0m)   â”‚  \u001b[32m2,170,496\u001b[0m â”‚ add_1[\u001b[32m0\u001b[0m][\u001b[32m0\u001b[0m]       â”‚\n",
            "â”‚ (\u001b[94mTransformerBlock\u001b[0m)  â”‚                   â”‚            â”‚                   â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ add_2 (\u001b[94mAdd\u001b[0m)         â”‚ (\u001b[96mNone\u001b[0m, \u001b[32m14\u001b[0m, \u001b[32m256\u001b[0m)   â”‚          \u001b[32m0\u001b[0m â”‚ transformer_blocâ€¦ â”‚\n",
            "â”‚                     â”‚                   â”‚            â”‚ transformer_blocâ€¦ â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ transformer_block_7 â”‚ (\u001b[96mNone\u001b[0m, \u001b[32m14\u001b[0m, \u001b[32m256\u001b[0m)   â”‚  \u001b[32m2,170,496\u001b[0m â”‚ add_2[\u001b[32m0\u001b[0m][\u001b[32m0\u001b[0m]       â”‚\n",
            "â”‚ (\u001b[94mTransformerBlock\u001b[0m)  â”‚                   â”‚            â”‚                   â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ conv1d_transpose    â”‚ (\u001b[96mNone\u001b[0m, \u001b[32m28\u001b[0m, \u001b[32m128\u001b[0m)   â”‚    \u001b[32m360,576\u001b[0m â”‚ transformer_blocâ€¦ â”‚\n",
            "â”‚ (\u001b[94mConv1DTranspose\u001b[0m)   â”‚                   â”‚            â”‚                   â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ concatenate         â”‚ (\u001b[96mNone\u001b[0m, \u001b[32m28\u001b[0m, \u001b[32m384\u001b[0m)   â”‚          \u001b[32m0\u001b[0m â”‚ conv1d_transposeâ€¦ â”‚\n",
            "â”‚ (\u001b[94mConcatenate\u001b[0m)       â”‚                   â”‚            â”‚ dropout_2[\u001b[32m0\u001b[0m][\u001b[32m0\u001b[0m]   â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ conv1d_11 (\u001b[94mConv1D\u001b[0m)  â”‚ (\u001b[96mNone\u001b[0m, \u001b[32m28\u001b[0m, \u001b[32m128\u001b[0m)   â”‚    \u001b[32m540,800\u001b[0m â”‚ concatenate[\u001b[32m0\u001b[0m][\u001b[32m0\u001b[0m] â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ dcb_1__0            â”‚ (\u001b[96mNone\u001b[0m, \u001b[32m28\u001b[0m, \u001b[32m128\u001b[0m)   â”‚     \u001b[32m16,576\u001b[0m â”‚ conv1d_11[\u001b[32m0\u001b[0m][\u001b[32m0\u001b[0m]   â”‚\n",
            "â”‚ (\u001b[94mSqueezeExcitationâ€¦\u001b[0m â”‚                   â”‚            â”‚                   â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ batch_normalizatioâ€¦ â”‚ (\u001b[96mNone\u001b[0m, \u001b[32m28\u001b[0m, \u001b[32m128\u001b[0m)   â”‚        \u001b[32m512\u001b[0m â”‚ dcb_1__0[\u001b[32m0\u001b[0m][\u001b[32m0\u001b[0m]    â”‚\n",
            "â”‚ (\u001b[94mBatchNormalizatioâ€¦\u001b[0m â”‚                   â”‚            â”‚                   â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ conv1d_12 (\u001b[94mConv1D\u001b[0m)  â”‚ (\u001b[96mNone\u001b[0m, \u001b[32m28\u001b[0m, \u001b[32m128\u001b[0m)   â”‚    \u001b[32m180,352\u001b[0m â”‚ batch_normalizatâ€¦ â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ dcb_1__1            â”‚ (\u001b[96mNone\u001b[0m, \u001b[32m28\u001b[0m, \u001b[32m128\u001b[0m)   â”‚     \u001b[32m16,576\u001b[0m â”‚ conv1d_12[\u001b[32m0\u001b[0m][\u001b[32m0\u001b[0m]   â”‚\n",
            "â”‚ (\u001b[94mSqueezeExcitationâ€¦\u001b[0m â”‚                   â”‚            â”‚                   â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ batch_normalizatioâ€¦ â”‚ (\u001b[96mNone\u001b[0m, \u001b[32m28\u001b[0m, \u001b[32m128\u001b[0m)   â”‚        \u001b[32m512\u001b[0m â”‚ dcb_1__1[\u001b[32m0\u001b[0m][\u001b[32m0\u001b[0m]    â”‚\n",
            "â”‚ (\u001b[94mBatchNormalizatioâ€¦\u001b[0m â”‚                   â”‚            â”‚                   â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ conv1d_transpose_1  â”‚ (\u001b[96mNone\u001b[0m, \u001b[32m56\u001b[0m, \u001b[32m128\u001b[0m)   â”‚    \u001b[32m180,352\u001b[0m â”‚ batch_normalizatâ€¦ â”‚\n",
            "â”‚ (\u001b[94mConv1DTranspose\u001b[0m)   â”‚                   â”‚            â”‚                   â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ concatenate_1       â”‚ (\u001b[96mNone\u001b[0m, \u001b[32m56\u001b[0m, \u001b[32m256\u001b[0m)   â”‚          \u001b[32m0\u001b[0m â”‚ conv1d_transposeâ€¦ â”‚\n",
            "â”‚ (\u001b[94mConcatenate\u001b[0m)       â”‚                   â”‚            â”‚ dropout_1[\u001b[32m0\u001b[0m][\u001b[32m0\u001b[0m]   â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ conv1d_13 (\u001b[94mConv1D\u001b[0m)  â”‚ (\u001b[96mNone\u001b[0m, \u001b[32m56\u001b[0m, \u001b[32m128\u001b[0m)   â”‚    \u001b[32m360,576\u001b[0m â”‚ concatenate_1[\u001b[32m0\u001b[0m]â€¦ â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ dcb_2__0            â”‚ (\u001b[96mNone\u001b[0m, \u001b[32m56\u001b[0m, \u001b[32m128\u001b[0m)   â”‚     \u001b[32m16,576\u001b[0m â”‚ conv1d_13[\u001b[32m0\u001b[0m][\u001b[32m0\u001b[0m]   â”‚\n",
            "â”‚ (\u001b[94mSqueezeExcitationâ€¦\u001b[0m â”‚                   â”‚            â”‚                   â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ batch_normalizatioâ€¦ â”‚ (\u001b[96mNone\u001b[0m, \u001b[32m56\u001b[0m, \u001b[32m128\u001b[0m)   â”‚        \u001b[32m512\u001b[0m â”‚ dcb_2__0[\u001b[32m0\u001b[0m][\u001b[32m0\u001b[0m]    â”‚\n",
            "â”‚ (\u001b[94mBatchNormalizatioâ€¦\u001b[0m â”‚                   â”‚            â”‚                   â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ conv1d_14 (\u001b[94mConv1D\u001b[0m)  â”‚ (\u001b[96mNone\u001b[0m, \u001b[32m56\u001b[0m, \u001b[32m128\u001b[0m)   â”‚    \u001b[32m180,352\u001b[0m â”‚ batch_normalizatâ€¦ â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ dcb_2__1            â”‚ (\u001b[96mNone\u001b[0m, \u001b[32m56\u001b[0m, \u001b[32m128\u001b[0m)   â”‚     \u001b[32m16,576\u001b[0m â”‚ conv1d_14[\u001b[32m0\u001b[0m][\u001b[32m0\u001b[0m]   â”‚\n",
            "â”‚ (\u001b[94mSqueezeExcitationâ€¦\u001b[0m â”‚                   â”‚            â”‚                   â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ batch_normalizatioâ€¦ â”‚ (\u001b[96mNone\u001b[0m, \u001b[32m56\u001b[0m, \u001b[32m128\u001b[0m)   â”‚        \u001b[32m512\u001b[0m â”‚ dcb_2__1[\u001b[32m0\u001b[0m][\u001b[32m0\u001b[0m]    â”‚\n",
            "â”‚ (\u001b[94mBatchNormalizatioâ€¦\u001b[0m â”‚                   â”‚            â”‚                   â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ conv1d_transpose_2  â”‚ (\u001b[96mNone\u001b[0m, \u001b[32m112\u001b[0m, \u001b[32m128\u001b[0m)  â”‚    \u001b[32m180,352\u001b[0m â”‚ batch_normalizatâ€¦ â”‚\n",
            "â”‚ (\u001b[94mConv1DTranspose\u001b[0m)   â”‚                   â”‚            â”‚                   â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ concatenate_2       â”‚ (\u001b[96mNone\u001b[0m, \u001b[32m112\u001b[0m, \u001b[32m256\u001b[0m)  â”‚          \u001b[32m0\u001b[0m â”‚ conv1d_transposeâ€¦ â”‚\n",
            "â”‚ (\u001b[94mConcatenate\u001b[0m)       â”‚                   â”‚            â”‚ dropout[\u001b[32m0\u001b[0m][\u001b[32m0\u001b[0m]     â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ conv1d_15 (\u001b[94mConv1D\u001b[0m)  â”‚ (\u001b[96mNone\u001b[0m, \u001b[32m112\u001b[0m, \u001b[32m128\u001b[0m)  â”‚    \u001b[32m360,576\u001b[0m â”‚ concatenate_2[\u001b[32m0\u001b[0m]â€¦ â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ dcb_3__0            â”‚ (\u001b[96mNone\u001b[0m, \u001b[32m112\u001b[0m, \u001b[32m128\u001b[0m)  â”‚     \u001b[32m16,576\u001b[0m â”‚ conv1d_15[\u001b[32m0\u001b[0m][\u001b[32m0\u001b[0m]   â”‚\n",
            "â”‚ (\u001b[94mSqueezeExcitationâ€¦\u001b[0m â”‚                   â”‚            â”‚                   â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ batch_normalizatioâ€¦ â”‚ (\u001b[96mNone\u001b[0m, \u001b[32m112\u001b[0m, \u001b[32m128\u001b[0m)  â”‚        \u001b[32m512\u001b[0m â”‚ dcb_3__0[\u001b[32m0\u001b[0m][\u001b[32m0\u001b[0m]    â”‚\n",
            "â”‚ (\u001b[94mBatchNormalizatioâ€¦\u001b[0m â”‚                   â”‚            â”‚                   â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ conv1d_16 (\u001b[94mConv1D\u001b[0m)  â”‚ (\u001b[96mNone\u001b[0m, \u001b[32m112\u001b[0m, \u001b[32m128\u001b[0m)  â”‚    \u001b[32m180,352\u001b[0m â”‚ batch_normalizatâ€¦ â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ dcb_3__1            â”‚ (\u001b[96mNone\u001b[0m, \u001b[32m112\u001b[0m, \u001b[32m128\u001b[0m)  â”‚     \u001b[32m16,576\u001b[0m â”‚ conv1d_16[\u001b[32m0\u001b[0m][\u001b[32m0\u001b[0m]   â”‚\n",
            "â”‚ (\u001b[94mSqueezeExcitationâ€¦\u001b[0m â”‚                   â”‚            â”‚                   â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ batch_normalizatioâ€¦ â”‚ (\u001b[96mNone\u001b[0m, \u001b[32m112\u001b[0m, \u001b[32m128\u001b[0m)  â”‚        \u001b[32m512\u001b[0m â”‚ dcb_3__1[\u001b[32m0\u001b[0m][\u001b[32m0\u001b[0m]    â”‚\n",
            "â”‚ (\u001b[94mBatchNormalizatioâ€¦\u001b[0m â”‚                   â”‚            â”‚                   â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ conv1d_17 (\u001b[94mConv1D\u001b[0m)  â”‚ (\u001b[96mNone\u001b[0m, \u001b[32m112\u001b[0m, \u001b[32m128\u001b[0m)  â”‚    \u001b[32m180,352\u001b[0m â”‚ batch_normalizatâ€¦ â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ se_0                â”‚ (\u001b[96mNone\u001b[0m, \u001b[32m112\u001b[0m, \u001b[32m128\u001b[0m)  â”‚     \u001b[32m16,576\u001b[0m â”‚ conv1d_17[\u001b[32m0\u001b[0m][\u001b[32m0\u001b[0m]   â”‚\n",
            "â”‚ (\u001b[94mSqueezeExcitationâ€¦\u001b[0m â”‚                   â”‚            â”‚                   â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ conv1d_18 (\u001b[94mConv1D\u001b[0m)  â”‚ (\u001b[96mNone\u001b[0m, \u001b[32m112\u001b[0m, \u001b[32m64\u001b[0m)   â”‚     \u001b[32m90,176\u001b[0m â”‚ se_0[\u001b[32m0\u001b[0m][\u001b[32m0\u001b[0m]        â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ se_1                â”‚ (\u001b[96mNone\u001b[0m, \u001b[32m112\u001b[0m, \u001b[32m64\u001b[0m)   â”‚      \u001b[32m4,192\u001b[0m â”‚ conv1d_18[\u001b[32m0\u001b[0m][\u001b[32m0\u001b[0m]   â”‚\n",
            "â”‚ (\u001b[94mSqueezeExcitationâ€¦\u001b[0m â”‚                   â”‚            â”‚                   â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ conv1d_19 (\u001b[94mConv1D\u001b[0m)  â”‚ (\u001b[96mNone\u001b[0m, \u001b[32m112\u001b[0m, \u001b[32m32\u001b[0m)   â”‚     \u001b[32m22,560\u001b[0m â”‚ se_1[\u001b[32m0\u001b[0m][\u001b[32m0\u001b[0m]        â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ se_2                â”‚ (\u001b[96mNone\u001b[0m, \u001b[32m112\u001b[0m, \u001b[32m32\u001b[0m)   â”‚      \u001b[32m1,072\u001b[0m â”‚ conv1d_19[\u001b[32m0\u001b[0m][\u001b[32m0\u001b[0m]   â”‚\n",
            "â”‚ (\u001b[94mSqueezeExcitationâ€¦\u001b[0m â”‚                   â”‚            â”‚                   â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ flatten (\u001b[94mFlatten\u001b[0m)   â”‚ (\u001b[96mNone\u001b[0m, \u001b[32m3584\u001b[0m)      â”‚          \u001b[32m0\u001b[0m â”‚ se_2[\u001b[32m0\u001b[0m][\u001b[32m0\u001b[0m]        â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ dense_50 (\u001b[94mDense\u001b[0m)    â”‚ (\u001b[96mNone\u001b[0m, \u001b[32m256\u001b[0m)       â”‚    \u001b[32m917,760\u001b[0m â”‚ flatten[\u001b[32m0\u001b[0m][\u001b[32m0\u001b[0m]     â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ re_lu (\u001b[94mReLU\u001b[0m)        â”‚ (\u001b[96mNone\u001b[0m, \u001b[32m256\u001b[0m)       â”‚          \u001b[32m0\u001b[0m â”‚ dense_50[\u001b[32m0\u001b[0m][\u001b[32m0\u001b[0m]    â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ dropout_28          â”‚ (\u001b[96mNone\u001b[0m, \u001b[32m256\u001b[0m)       â”‚          \u001b[32m0\u001b[0m â”‚ re_lu[\u001b[32m0\u001b[0m][\u001b[32m0\u001b[0m]       â”‚\n",
            "â”‚ (\u001b[94mDropout\u001b[0m)           â”‚                   â”‚            â”‚                   â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ dense_51 (\u001b[94mDense\u001b[0m)    â”‚ (\u001b[96mNone\u001b[0m, \u001b[32m128\u001b[0m)       â”‚     \u001b[32m32,896\u001b[0m â”‚ dropout_28[\u001b[32m0\u001b[0m][\u001b[32m0\u001b[0m]  â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ re_lu_1 (\u001b[94mReLU\u001b[0m)      â”‚ (\u001b[96mNone\u001b[0m, \u001b[32m128\u001b[0m)       â”‚          \u001b[32m0\u001b[0m â”‚ dense_51[\u001b[32m0\u001b[0m][\u001b[32m0\u001b[0m]    â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ dropout_29          â”‚ (\u001b[96mNone\u001b[0m, \u001b[32m128\u001b[0m)       â”‚          \u001b[32m0\u001b[0m â”‚ re_lu_1[\u001b[32m0\u001b[0m][\u001b[32m0\u001b[0m]     â”‚\n",
            "â”‚ (\u001b[94mDropout\u001b[0m)           â”‚                   â”‚            â”‚                   â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ dense_52 (\u001b[94mDense\u001b[0m)    â”‚ (\u001b[96mNone\u001b[0m, \u001b[32m64\u001b[0m)        â”‚      \u001b[32m8,256\u001b[0m â”‚ dropout_29[\u001b[32m0\u001b[0m][\u001b[32m0\u001b[0m]  â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ re_lu_2 (\u001b[94mReLU\u001b[0m)      â”‚ (\u001b[96mNone\u001b[0m, \u001b[32m64\u001b[0m)        â”‚          \u001b[32m0\u001b[0m â”‚ dense_52[\u001b[32m0\u001b[0m][\u001b[32m0\u001b[0m]    â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ dropout_30          â”‚ (\u001b[96mNone\u001b[0m, \u001b[32m64\u001b[0m)        â”‚          \u001b[32m0\u001b[0m â”‚ re_lu_2[\u001b[32m0\u001b[0m][\u001b[32m0\u001b[0m]     â”‚\n",
            "â”‚ (\u001b[94mDropout\u001b[0m)           â”‚                   â”‚            â”‚                   â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ dense_53 (\u001b[94mDense\u001b[0m)    â”‚ (\u001b[96mNone\u001b[0m, \u001b[32m32\u001b[0m)        â”‚      \u001b[32m2,080\u001b[0m â”‚ dropout_30[\u001b[32m0\u001b[0m][\u001b[32m0\u001b[0m]  â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ re_lu_3 (\u001b[94mReLU\u001b[0m)      â”‚ (\u001b[96mNone\u001b[0m, \u001b[32m32\u001b[0m)        â”‚          \u001b[32m0\u001b[0m â”‚ dense_53[\u001b[32m0\u001b[0m][\u001b[32m0\u001b[0m]    â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ dropout_31          â”‚ (\u001b[96mNone\u001b[0m, \u001b[32m32\u001b[0m)        â”‚          \u001b[32m0\u001b[0m â”‚ re_lu_3[\u001b[32m0\u001b[0m][\u001b[32m0\u001b[0m]     â”‚\n",
            "â”‚ (\u001b[94mDropout\u001b[0m)           â”‚                   â”‚            â”‚                   â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ dense_54 (\u001b[94mDense\u001b[0m)    â”‚ (\u001b[96mNone\u001b[0m, \u001b[32m1\u001b[0m)         â”‚         \u001b[32m33\u001b[0m â”‚ dropout_31[\u001b[32m0\u001b[0m][\u001b[32m0\u001b[0m]  â”‚\n",
            "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
            "\u001b[1m Total params: \u001b[0m\u001b[32m27,191,857\u001b[0m (103.73 MB)\n",
            "\u001b[1m Trainable params: \u001b[0m\u001b[32m27,187,249\u001b[0m (103.71 MB)\n",
            "\u001b[1m Non-trainable params: \u001b[0m\u001b[32m4,608\u001b[0m (18.00 KB)\n",
            "\n",
            "âš™ï¸  Callbacks hazÄ±rlanÄ±yor...\n",
            "âœ… Callbacks hazÄ±r\n",
            "\n",
            "============================================================\n",
            "ðŸš€ EÄŸitim baÅŸlÄ±yor...\n",
            "============================================================\n",
            "ðŸ“Š Epochs: 85\n",
            "ðŸ“¦ Batch size: 8\n",
            "ðŸŽ¯ Input shape: (None, 112, 1024)\n",
            "============================================================\n",
            "\n",
            "2026-02-15 16:50:19.442769: W tensorflow/core/kernels/data/cache_dataset_ops.cc:916] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
            "Epoch 1/85\n",
            "I0000 00:00:1771174247.114568   27406 cuda_dnn.cc:529] Loaded cuDNN version 91002\n",
            "2026-02-15 16:52:04.129499: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
            "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n",
            "2026-02-15 16:52:04.129561: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
            "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n",
            "\t [[RemoteCall]]\n",
            "2026-02-15 16:52:04.422287: W tensorflow/core/kernels/data/cache_dataset_ops.cc:916] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
            "2026-02-15 16:52:10.719164: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
            "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n",
            "\t [[RemoteCall]]\n",
            "\n",
            "Epoch 1: val_auc improved from -inf to 0.97529, saving model to saved_model/model/DeepAllergen_best.h5\n",
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
            "631/631 - 121s - 192ms/step - accuracy: 0.8035 - auc: 0.8944 - loss: 1.4257 - val_accuracy: 0.9241 - val_auc: 0.9753 - val_loss: 1.2192 - learning_rate: 1.0000e-04\n",
            "Epoch 2/85\n",
            "2026-02-15 16:53:38.720664: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
            "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n",
            "\t [[RemoteCall]]\n",
            "\n",
            "Epoch 2: val_auc did not improve from 0.97529\n",
            "631/631 - 86s - 136ms/step - accuracy: 0.9241 - auc: 0.9638 - loss: 1.1623 - val_accuracy: 0.9306 - val_auc: 0.9724 - val_loss: 1.0795 - learning_rate: 1.2000e-04\n",
            "Epoch 3/85\n",
            "\n",
            "Epoch 3: val_auc did not improve from 0.97529\n",
            "631/631 - 79s - 126ms/step - accuracy: 0.9469 - auc: 0.9772 - loss: 0.9835 - val_accuracy: 0.9306 - val_auc: 0.9482 - val_loss: 1.0110 - learning_rate: 1.4000e-04\n",
            "Epoch 4/85\n",
            "2026-02-15 16:56:18.065396: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
            "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n",
            "\t [[RemoteCall]]\n",
            "\n",
            "Epoch 4: val_auc improved from 0.97529 to 0.97530, saving model to saved_model/model/DeepAllergen_best.h5\n",
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
            "631/631 - 83s - 132ms/step - accuracy: 0.9584 - auc: 0.9829 - loss: 0.8586 - val_accuracy: 0.9426 - val_auc: 0.9753 - val_loss: 0.8428 - learning_rate: 1.6000e-04\n",
            "Epoch 5/85\n",
            "\n",
            "Epoch 5: val_auc did not improve from 0.97530\n",
            "631/631 - 78s - 124ms/step - accuracy: 0.9685 - auc: 0.9900 - loss: 0.7502 - val_accuracy: 0.9333 - val_auc: 0.9700 - val_loss: 0.7823 - learning_rate: 1.8000e-04\n",
            "Epoch 6/85\n",
            "\n",
            "Epoch 6: val_auc did not improve from 0.97530\n",
            "631/631 - 81s - 128ms/step - accuracy: 0.9739 - auc: 0.9931 - loss: 0.6662 - val_accuracy: 0.9407 - val_auc: 0.9738 - val_loss: 0.7072 - learning_rate: 2.0000e-04\n",
            "Epoch 7/85\n",
            "\n",
            "Epoch 7: val_auc did not improve from 0.97530\n",
            "631/631 - 78s - 123ms/step - accuracy: 0.9784 - auc: 0.9946 - loss: 0.5965 - val_accuracy: 0.9278 - val_auc: 0.9685 - val_loss: 0.6753 - learning_rate: 2.2000e-04\n",
            "Epoch 8/85\n",
            "2026-02-15 17:01:37.348016: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
            "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n",
            "\t [[RemoteCall]]\n",
            "\n",
            "Epoch 8: val_auc did not improve from 0.97530\n",
            "631/631 - 78s - 123ms/step - accuracy: 0.9840 - auc: 0.9953 - loss: 0.5269 - val_accuracy: 0.9435 - val_auc: 0.9705 - val_loss: 0.6051 - learning_rate: 2.4000e-04\n",
            "Epoch 9/85\n",
            "\n",
            "Epoch 9: val_auc did not improve from 0.97530\n",
            "631/631 - 78s - 124ms/step - accuracy: 0.9800 - auc: 0.9947 - loss: 0.4800 - val_accuracy: 0.9231 - val_auc: 0.9706 - val_loss: 0.5731 - learning_rate: 2.6000e-04\n",
            "Epoch 10/85\n",
            "\n",
            "Epoch 10: val_auc did not improve from 0.97530\n",
            "631/631 - 79s - 126ms/step - accuracy: 0.9818 - auc: 0.9963 - loss: 0.4262 - val_accuracy: 0.9380 - val_auc: 0.9736 - val_loss: 0.4959 - learning_rate: 2.8000e-04\n",
            "Epoch 11/85\n",
            "\n",
            "Epoch 11: val_auc did not improve from 0.97530\n",
            "631/631 - 79s - 125ms/step - accuracy: 0.9834 - auc: 0.9968 - loss: 0.3759 - val_accuracy: 0.9306 - val_auc: 0.9682 - val_loss: 0.4759 - learning_rate: 3.0000e-04\n",
            "Epoch 12/85\n",
            "\n",
            "Epoch 12: val_auc did not improve from 0.97530\n",
            "631/631 - 78s - 124ms/step - accuracy: 0.9824 - auc: 0.9969 - loss: 0.3600 - val_accuracy: 0.9287 - val_auc: 0.9604 - val_loss: 0.5100 - learning_rate: 2.9987e-04\n",
            "Epoch 13/85\n",
            "\n",
            "Epoch 13: val_auc did not improve from 0.97530\n",
            "631/631 - 79s - 125ms/step - accuracy: 0.9836 - auc: 0.9979 - loss: 0.3404 - val_accuracy: 0.9417 - val_auc: 0.9673 - val_loss: 0.4398 - learning_rate: 2.9948e-04\n",
            "Epoch 14/85\n",
            "\n",
            "Epoch 14: val_auc did not improve from 0.97530\n",
            "631/631 - 79s - 126ms/step - accuracy: 0.9909 - auc: 0.9980 - loss: 0.3066 - val_accuracy: 0.9398 - val_auc: 0.9702 - val_loss: 0.4112 - learning_rate: 2.9883e-04\n",
            "Epoch 15/85\n",
            "\n",
            "Epoch 15: val_auc did not improve from 0.97530\n",
            "631/631 - 79s - 126ms/step - accuracy: 0.9929 - auc: 0.9996 - loss: 0.2660 - val_accuracy: 0.9343 - val_auc: 0.9691 - val_loss: 0.3914 - learning_rate: 2.9791e-04\n",
            "Epoch 16/85\n",
            "2026-02-15 17:12:08.761842: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
            "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n",
            "\t [[RemoteCall]]\n",
            "\n",
            "Epoch 16: val_auc did not improve from 0.97530\n",
            "631/631 - 80s - 126ms/step - accuracy: 0.9923 - auc: 0.9988 - loss: 0.2475 - val_accuracy: 0.9463 - val_auc: 0.9705 - val_loss: 0.3718 - learning_rate: 2.9675e-04\n",
            "Epoch 17/85\n",
            "\n",
            "Epoch 17: val_auc did not improve from 0.97530\n",
            "631/631 - 79s - 125ms/step - accuracy: 0.9855 - auc: 0.9984 - loss: 0.2845 - val_accuracy: 0.9352 - val_auc: 0.9689 - val_loss: 0.4222 - learning_rate: 2.9532e-04\n",
            "Epoch 18/85\n",
            "\n",
            "Epoch 18: val_auc did not improve from 0.97530\n",
            "631/631 - 79s - 126ms/step - accuracy: 0.9891 - auc: 0.9985 - loss: 0.2810 - val_accuracy: 0.9417 - val_auc: 0.9730 - val_loss: 0.3924 - learning_rate: 2.9364e-04\n",
            "Epoch 19/85\n",
            "\n",
            "Epoch 19: ReduceLROnPlateau reducing learning rate to 0.00014585861936211586.\n",
            "\n",
            "Epoch 19: val_auc did not improve from 0.97530\n",
            "631/631 - 80s - 126ms/step - accuracy: 0.9891 - auc: 0.9976 - loss: 0.2865 - val_accuracy: 0.9343 - val_auc: 0.9668 - val_loss: 0.4256 - learning_rate: 2.9172e-04\n",
            "Epoch 20/85\n",
            "\n",
            "Epoch 20: val_auc improved from 0.97530 to 0.97690, saving model to saved_model/model/DeepAllergen_best.h5\n",
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
            "631/631 - 84s - 133ms/step - accuracy: 0.9891 - auc: 0.9986 - loss: 0.2901 - val_accuracy: 0.9444 - val_auc: 0.9769 - val_loss: 0.3888 - learning_rate: 2.8954e-04\n",
            "Epoch 21/85\n",
            "\n",
            "Epoch 21: val_auc did not improve from 0.97690\n",
            "631/631 - 81s - 129ms/step - accuracy: 0.9947 - auc: 0.9995 - loss: 0.2617 - val_accuracy: 0.9380 - val_auc: 0.9648 - val_loss: 0.3851 - learning_rate: 2.8713e-04\n",
            "Epoch 22/85\n",
            "\n",
            "Epoch 22: ReduceLROnPlateau reducing learning rate to 0.0001422371278749779.\n",
            "\n",
            "Epoch 22: val_auc did not improve from 0.97690\n",
            "631/631 - 87s - 138ms/step - accuracy: 0.9956 - auc: 0.9996 - loss: 0.2363 - val_accuracy: 0.9324 - val_auc: 0.9645 - val_loss: 0.3896 - learning_rate: 2.8447e-04\n",
            "Epoch 23/85\n",
            "\n",
            "Epoch 23: val_auc did not improve from 0.97690\n",
            "631/631 - 81s - 128ms/step - accuracy: 0.9915 - auc: 0.9991 - loss: 0.2410 - val_accuracy: 0.9324 - val_auc: 0.9704 - val_loss: 0.3872 - learning_rate: 2.8159e-04\n",
            "Epoch 24/85\n",
            "\n",
            "Epoch 24: val_auc did not improve from 0.97690\n",
            "631/631 - 81s - 128ms/step - accuracy: 0.9937 - auc: 0.9982 - loss: 0.2345 - val_accuracy: 0.9231 - val_auc: 0.9697 - val_loss: 0.3927 - learning_rate: 2.7847e-04\n",
            "Epoch 24: early stopping\n",
            "Restoring model weights from the end of the best epoch: 16.\n",
            "\n",
            "============================================================\n",
            "âœ… EÄŸitim tamamlandÄ±!\n",
            "============================================================\n",
            "â±ï¸  Toplam sÃ¼re: 32.88 dakika\n",
            "============================================================\n",
            "\n",
            "ðŸ’¾ Model kaydediliyor...\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/train.py\", line 189, in <module>\n",
            "    model.save(os.path.join(model_path, model_arch))\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/keras/src/utils/traceback_utils.py\", line 122, in error_handler\n",
            "    raise e.with_traceback(filtered_tb) from None\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/keras/src/saving/saving_api.py\", line 114, in save_model\n",
            "    raise ValueError(\n",
            "ValueError: Invalid filepath extension for saving. Please add either a `.keras` extension for the native Keras format (recommended) or a `.h5` extension. Use `model.export(filepath)` if you want to export a SavedModel for use with TFLite/TFServing/etc. Received: filepath=saved_model/model/DeepAllergen.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python test.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jL1dGVaw7g26",
        "outputId": "27336d68-dd82-4ce6-b9d3-6808a4c4871a"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2026-02-15 17:23:49.660751: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1771176229.691300   37467 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1771176229.701672   37467 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1771176229.728215   37467 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1771176229.728250   37467 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1771176229.728258   37467 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1771176229.728265   37467 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2026-02-15 17:23:49.735208: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\n",
            "ðŸ“‚ Loading test dataset...\n",
            "WARNING:tensorflow:From /content/test.py:19: load (from tensorflow.python.data.experimental.ops.io) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.load(...)` instead.\n",
            "2026-02-15 17:23:58.298932: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "I0000 00:00:1771176238.300506   37467 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13757 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "âœ… Test dataset loaded\n",
            "\n",
            "ðŸ—ï¸ Loading model...\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/test.py\", line 41, in <module>\n",
            "    \"SqueezeExcitationBlock\": SqueezeExcitationBlock\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^\n",
            "NameError: name 'SqueezeExcitationBlock' is not defined. Did you mean: 'SqueezeExcitation1DLayer'?\n"
          ]
        }
      ]
    }
  ]
}